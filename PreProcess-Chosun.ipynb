{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Data PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined Modules\n",
    "from myModules.utils.data.DataLoader import DataLoader\n",
    "from myModules.utils.merge.mergeOverPeriod import merge\n",
    "from myModules.preprocess import cleaning, removeStopWords_ST, tagging\n",
    "\n",
    "# General Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read File\n",
    "import glob\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = './Data/조선사진첩(1925) 영문 텍스트.txt'\n",
    "\n",
    "RESULT_ROOT = './Result/chosun/'\n",
    "\n",
    "with open(DATA_ROOT, 'r', encoding='UTF-16') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = [text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Data Cleaning\n",
    "\n",
    "- `dot(.)`과 `apostrophe(')`는 제거하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaning(texts=text, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dot_and_apostrophe:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def token_with_apostrophe(self):\n",
    "        apostrophe = []\n",
    "\n",
    "        for tokens in self.data:\n",
    "            for token in tokens:\n",
    "                if \"'\" in token : apostrophe.append(token)\n",
    "        \n",
    "        self.apostrophes = set(apostrophe)\n",
    "\n",
    "        print(f\"apostrophe를 가진 token : \\n{self.apostrophes}\")\n",
    "    \n",
    "    def token_with_dot(self):\n",
    "        dot = []\n",
    "\n",
    "        for tokens in self.data:\n",
    "            for token in tokens:\n",
    "                if \".\" in token : dot.append(token)\n",
    "        \n",
    "        self.dots = set(dot)\n",
    "\n",
    "        print(f\"dot을 가진 token : \\n{self.dots}\")\n",
    "        \n",
    "    def set_exception(self, apostrophe_exception, dot_exception):\n",
    "        self.apostrophe_exception = apostrophe_exception\n",
    "        self.dot_exception = dot_exception\n",
    "    \n",
    "    def print_exception(self):\n",
    "        print(f\"apostrophe exceptions : \\n{self.apostrophe_exception}\")\n",
    "        print(f\"dot exceptions : \\n{self.dot_exception}\")\n",
    "    \n",
    "    def remove_apostrophe(self, data):\n",
    "        result = []\n",
    "        processed = []\n",
    "\n",
    "        for tokens in data:\n",
    "            arr = []\n",
    "            for token in tokens:\n",
    "                if token not in self.apostrophe_exception:\n",
    "                    if not token.isalnum() : \n",
    "                        if \".\" not in token : processed.append(token)\n",
    "                    # dot은 삭제하지 않음. -> 예외처리하면서 삭제해야함\n",
    "                    arr.append(re.sub(\"[^a-z\\.]\", \"\", token))\n",
    "                else : arr.append(token)\n",
    "            result.append(arr)\n",
    "        \n",
    "        processed = set(processed)\n",
    "\n",
    "        print(f\"Processed Tokens : \\n{processed}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def remove_dot(self, data):\n",
    "        result = []\n",
    "        processed = []\n",
    "\n",
    "        for tokens in data:\n",
    "            arr = []\n",
    "            for token in tokens:\n",
    "                if token not in self.dot_exception:\n",
    "                    if not token.isalnum() : \n",
    "                        if \"'\" not in token : processed.append(token)\n",
    "                    # apostrophe는 삭제하지 않음. -> 예외처리하면서 삭제\n",
    "                    arr.append(re.sub(\"[^a-z']\", \"\", token))\n",
    "                else : arr.append(token)\n",
    "            result.append(arr)\n",
    "        \n",
    "        processed = set(processed)\n",
    "\n",
    "        print(f\"Processed Tokens : \\n{processed}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_invalid_tokens(self, data):\n",
    "        # 예외처리한 Token들 외에 특수문자를 가진 Token들이 있는지 확인\n",
    "        invalid_tokens = []\n",
    "\n",
    "        for tokens in data:\n",
    "            for token in tokens:\n",
    "                if not token.isalnum() : invalid_tokens.append(token)\n",
    "                elif len(token) == 1 : invalid_tokens.append(token)\n",
    "        \n",
    "        invalid_tokens = set(invalid_tokens)\n",
    "        exception = set(self.apostrophe_exception).union(set(self.dot_exception))\n",
    "        self.invalid_symbol = invalid_tokens.difference(exception)\n",
    "\n",
    "        if len(self.invalid_symbol) == 0:\n",
    "            print(\"There is no invalid symbol\")\n",
    "        else :\n",
    "            print(f\"Remaining invalid Symbol : {self.invalid_symbol}\")\n",
    "    \n",
    "    def remove_invalid_tokens(self, data):\n",
    "        # 남아있는 특수문자 + 길이가 1인 token들을 삭제\n",
    "        \n",
    "        result = []\n",
    "        removed = []\n",
    "\n",
    "        for tokens in data:\n",
    "            arr = []\n",
    "            for token in tokens:\n",
    "                if len(token) == 1 : removed.append(token)\n",
    "                elif token in self.invalid_symbol : removed.append(token)\n",
    "                else : arr.append(token)\n",
    "            result.append(arr)\n",
    "\n",
    "        removed = set(removed)\n",
    "        \n",
    "        print(f\"Removed Tokens : \\n{removed}\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [word_tokenize(text) for text in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"girls'higher\", \"'s\", \"'\"}\n",
      "dot을 가진 token : \n",
      "{'.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'s\"]\n",
    "dot_exception = []\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'s\"]\n",
      "dot exceptions : \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"girls'higher\", \"'\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_ = symbol.remove_apostrophe(data=tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', '.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized__ = symbol.remove_dot(data=tokenized_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 token 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'a'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 Token들 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'a'}\n"
     ]
    }
   ],
   "source": [
    "tokenized___ = symbol.remove_invalid_tokens(data=tokenized__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 invalid한 token이 있는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['would', 'could', 'might', 'need', 'can', 'must', \\\n",
    "    'one', 'two', 'upon', 'may', 'perhaps', 'living', 'seem', 'also', 'ii', 'ofthe',\n",
    "    'also', 'much', 'therefore', \"'ll\", \"'ve\", \"n't\"]\n",
    "\n",
    "wo_stopword = removeStopWords_ST(tokenized___, stopwords, new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_table = pd.read_pickle(\"processed-data/pos-table.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1d7f088b26491a8769d2c8378e3e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged = tagging(wo_stopword, mode='ST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_pos:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.dots = {}\n",
    "        self.apostrophes = {}\n",
    "        self.dots_wo = {}\n",
    "        self.apostrophes_wo = {}\n",
    "\n",
    "        arr_dot = []\n",
    "        arr_apostrophe = []\n",
    "\n",
    "        for tags in self.data:\n",
    "            for tag in tags:\n",
    "                if \".\" in tag[0] : arr_dot.append(tag[0])\n",
    "                elif \"'\" in tag[0] : arr_apostrophe.append(tag[0])\n",
    "        \n",
    "        for dot in set(arr_dot):\n",
    "            self.dots[dot] = set([tag[1] for tag in merge(self.data) if tag[0] == dot])\n",
    "        \n",
    "        for apos in set(arr_apostrophe):\n",
    "            self.apostrophes[apos] = set([tag[1] for tag in merge(self.data) if tag[0] == apos])\n",
    "        \n",
    "        for dot in set(arr_dot):\n",
    "            removed = nltk.pos_tag([re.sub(\"[^a-z]\", \"\", dot)])\n",
    "            self.dots_wo[removed[0][0]] = [removed[0][1]]\n",
    "        \n",
    "        for apos in set(arr_apostrophe):\n",
    "            removed = nltk.pos_tag([re.sub(\"[^a-z]\", \"\", apos)])\n",
    "            self.apostrophes_wo[removed[0][0]] = [removed[0][1]]\n",
    "        \n",
    "    \n",
    "    def pos_with_symbol(self):\n",
    "        print(f\"tagged token with apostrophe : \\n{self.apostrophes}\")\n",
    "        print(f\"tagged token with dot : \\n{self.dots}\") \n",
    "\n",
    "    def pos_without_symbol(self):\n",
    "        print(f\"tagged token without apostrophe : \\n{self.apostrophes_wo}\")\n",
    "        print(f\"tagged token without dot : \\n{self.dots_wo}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'s\": {'POS'}}\n",
      "tagged token with dot : \n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'s': ['NN']}\n",
      "tagged token without dot : \n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmatization:\n",
    "    def __init__(self, data, lemmatizer, pos_table, allowed_pos=['noun', 'verb', 'adjective', 'adverb']):\n",
    "        self.data = data\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.allowed_pos = []\n",
    "        for pos in allowed_pos:\n",
    "            if pos == 'noun' : self.allowed_pos.extend(pos_table.Eng_tag[0])\n",
    "            elif pos == 'verb' : self.allowed_pos.extend(pos_table.Eng_tag[2])\n",
    "            elif pos == 'adjective' : self.allowed_pos.extend(pos_table.Eng_tag[3])\n",
    "            elif pos == 'adverb' : self.allowed_pos.extend(pos_table.Eng_tag[4])\n",
    "\n",
    "    def lemmatize(self):\n",
    "        result = []\n",
    "\n",
    "        for tags in self.data:\n",
    "            arr = []\n",
    "            for token, pos in tags:\n",
    "                if pos in self.allowed_pos :\n",
    "                    pos_info = pos[0].lower()\n",
    "                    if pos_info == 'j' : pos_info = 'a'\n",
    "                    elif pos_info =='w' : pos_info = 'r'\n",
    "                    try : arr.append(self.lemmatizer.lemmatize(token, pos_info))\n",
    "                    except : print(token, pos, pos_info)\n",
    "            result.append(arr)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged, lemmatizer, pos_table, allowed_pos=['noun', 'verb', 'adjective', 'adverb'])\n",
    "lemmatized_all = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged, lemmatizer, pos_table, allowed_pos=['noun'])\n",
    "lemmatized_noun = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged, lemmatizer, pos_table, allowed_pos=['verb'])\n",
    "lemmatized_verb = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged, lemmatizer, pos_table, allowed_pos=['adjective'])\n",
    "lemmatized_adjective = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged, lemmatizer, pos_table, allowed_pos=['adverb'])\n",
    "lemmatized_adverb = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save PreProcessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_ROOT = './processed-data/chosun/'\n",
    "\n",
    "def to_pickle(data, file_name, root='./'):\n",
    "    with open(root+file_name+'.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatized data to pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_all, file_name=\"lemmatized-all\", root=SAVE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_noun, file_name=\"lemmatized-noun\", root=SAVE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_verb, file_name=\"lemmatized-verb\", root=SAVE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_adjective, file_name=\"lemmatized-adjective\", root=SAVE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_adverb, file_name=\"lemmatized-adverb\", root=SAVE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagged data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=tagged, file_name=\"tagged\", root=SAVE_ROOT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a3111211be4281f3a8c4a9b25563b8d253df502c7e31f5318895c1792a97cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
