{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Data PreProcessing\n",
    "\n",
    "## [주요 고려 사항]\n",
    "1. `dot(.)`과 `apostrophe(')` 처리\n",
    "    - 'u.s.'와 'u.s.s.r.'과 같은 약자처리를 어떻게 할 것인가?\n",
    "    - 'america's'와 같은 소유격을 어떻게 처리할 것인가?\n",
    "        1. 처음 Cleaning 때, `dot(.)`과 `apostrophe(')`는 제거하지 않음\n",
    "            - `dot(.)`\n",
    "                - 'u.s', 'u.s.s.r'과 같은 약자를 유지시키기 위한 처리\n",
    "            - `apostrophe(')`\n",
    "                - 'america's'와 같은 소유격을 유지시켜서 Tokenizing때 's를 분리시키기 위함.\n",
    "        2. Tokenizing 이후, `dot(.)`과 `apostrophe(')`를 유지시켜야 하는 Token들 외에는 특수문자 제거\n",
    "            1. `apostrophe(')`와 `dot(.)`을 가진 Token들을 출력해보고 유지시킬 Token들의 목록을 결정\n",
    "            2. `apostrophe(')`를 유지시킬 Token들 외의 모든 Token들에서 `apostrophe(')` 및 특수문자 제거\n",
    "                - `dot(.)`은 다음 단계에서 예외처리를 하며 제거해야 하므로, 이 단계에서는 모든 `dot(.)`을 유지시킴\n",
    "            3. `dot(.)`을 유지시킬 Token들 외의 모든 Token들에서 `dot(.)` 및 특수문자 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined Modules\n",
    "from myModules.utils.data.DataLoader import DataLoader\n",
    "from myModules.utils.merge.mergeOverPeriod import merge\n",
    "from myModules.preprocess import cleaning, removeStopWords_ST, tagging, extract_some_pos_ST\n",
    "\n",
    "# General Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read File\n",
    "import glob\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = './Data/3구간/'\n",
    "\n",
    "PERIOD_1 = DATA_ROOT + '1시기/1시기_ST/'\n",
    "PERIOD_2 = DATA_ROOT + '2시기/2시기_ST/'\n",
    "PERIOD_3 = DATA_ROOT + '3시기/3시기_ST/'\n",
    "\n",
    "RESULT_ROOT = './Result/3구간/'\n",
    "\n",
    "RESULT_1 = RESULT_ROOT + '/1시기/ST/'\n",
    "RESULT_2 = RESULT_ROOT + '/2시기/ST/'\n",
    "RESULT_3 = RESULT_ROOT + '/3시기/ST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_1 = glob.glob(PERIOD_1+'*.txt')\n",
    "files_2 = glob.glob(PERIOD_2+'*.txt')\n",
    "files_3 = glob.glob(PERIOD_3+'*.txt')\n",
    "\n",
    "texts_1 = DataLoader(files_1, mode='ST')\n",
    "texts_2 = DataLoader(files_2, mode='ST')\n",
    "texts_3 = DataLoader(files_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Data Cleaning\n",
    "\n",
    "- `dot(.)`과 `apostrophe(')`는 제거하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_1 = cleaning(texts=texts_1, mode='ST')\n",
    "cleaned_2 = cleaning(texts=texts_2, mode='ST')\n",
    "cleaned_3 = cleaning(texts=texts_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dot_and_apostrophe:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def token_with_apostrophe(self):\n",
    "        apostrophe = []\n",
    "\n",
    "        for tokens in self.data:\n",
    "            for token in tokens:\n",
    "                if \"'\" in token : apostrophe.append(token)\n",
    "        \n",
    "        self.apostrophes = set(apostrophe)\n",
    "\n",
    "        print(f\"apostrophe를 가진 token : \\n{self.apostrophes}\")\n",
    "    \n",
    "    def token_with_dot(self):\n",
    "        dot = []\n",
    "\n",
    "        for tokens in self.data:\n",
    "            for token in tokens:\n",
    "                if \".\" in token : dot.append(token)\n",
    "        \n",
    "        self.dots = set(dot)\n",
    "\n",
    "        print(f\"dot을 가진 token : \\n{self.dots}\")\n",
    "        \n",
    "    def set_exception(self, apostrophe_exception, dot_exception):\n",
    "        self.apostrophe_exception = apostrophe_exception\n",
    "        self.dot_exception = dot_exception\n",
    "    \n",
    "    def print_exception(self):\n",
    "        print(f\"apostrophe exceptions : \\n{self.apostrophe_exception}\")\n",
    "        print(f\"dot exceptions : \\n{self.dot_exception}\")\n",
    "    \n",
    "    def remove_apostrophe(self, data):\n",
    "        result = []\n",
    "        processed = []\n",
    "\n",
    "        for tokens in data:\n",
    "            arr = []\n",
    "            for token in tokens:\n",
    "                if token not in self.apostrophe_exception:\n",
    "                    if not token.isalnum() : \n",
    "                        if \".\" not in token : processed.append(token)\n",
    "                    # dot은 삭제하지 않음. -> 예외처리하면서 삭제해야함\n",
    "                    arr.append(re.sub(\"[^a-z\\.]\", \"\", token))\n",
    "                else : arr.append(token)\n",
    "            result.append(arr)\n",
    "        \n",
    "        processed = set(processed)\n",
    "\n",
    "        print(f\"Processed Tokens : \\n{processed}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def remove_dot(self, data):\n",
    "        result = []\n",
    "        processed = []\n",
    "\n",
    "        for tokens in data:\n",
    "            arr = []\n",
    "            for token in tokens:\n",
    "                if token not in self.dot_exception:\n",
    "                    if not token.isalnum() : \n",
    "                        if \"'\" not in token : processed.append(token)\n",
    "                    # apostrophe는 삭제하지 않음. -> 예외처리하면서 삭제\n",
    "                    arr.append(re.sub(\"[^a-z']\", \"\", token))\n",
    "                else : arr.append(token)\n",
    "            result.append(arr)\n",
    "        \n",
    "        processed = set(processed)\n",
    "\n",
    "        print(f\"Processed Tokens : \\n{processed}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_invalid_tokens(self, data):\n",
    "        # 예외처리한 Token들 외에 특수문자를 가진 Token들이 있는지 확인\n",
    "        invalid_tokens = []\n",
    "\n",
    "        for tokens in data:\n",
    "            for token in tokens:\n",
    "                if not token.isalnum() : invalid_tokens.append(token)\n",
    "                elif len(token) == 1 : invalid_tokens.append(token)\n",
    "        \n",
    "        invalid_tokens = set(invalid_tokens)\n",
    "        exception = set(self.apostrophe_exception).union(set(self.dot_exception))\n",
    "        self.invalid_symbol = invalid_tokens.difference(exception)\n",
    "\n",
    "        if len(self.invalid_symbol) == 0:\n",
    "            print(\"There is no invalid symbol\")\n",
    "        else :\n",
    "            print(f\"Remaining invalid Symbol : {self.invalid_symbol}\")\n",
    "    \n",
    "    def remove_invalid_tokens(self, data):\n",
    "        # 남아있는 특수문자 + 길이가 1인 token들을 삭제\n",
    "        \n",
    "        result = []\n",
    "        removed = []\n",
    "\n",
    "        for tokens in data:\n",
    "            arr = []\n",
    "            for token in tokens:\n",
    "                if len(token) == 1 : removed.append(token)\n",
    "                elif token in self.invalid_symbol : removed.append(token)\n",
    "                else : arr.append(token)\n",
    "            result.append(arr)\n",
    "\n",
    "        removed = set(removed)\n",
    "        \n",
    "        print(f\"Removed Tokens : \\n{removed}\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_1 = [word_tokenize(text) for text in cleaned_1]\n",
    "tokenized_2 = [word_tokenize(text) for text in cleaned_2]\n",
    "tokenized_3 = [word_tokenize(text) for text in cleaned_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'ve\", \"'ll\", \"'into\", \"o'clock\", \"'m\", \"'system\", \"'s\", \"'mvd\", \"'liberty\", \"'\", \"'german\", \"'are\", \"'d\", \"'blamed\", \"'madam\", \"'structure\", \"'democracy\", \"n't\", \"'heat\"}\n",
      "dot을 가진 token : \n",
      "{'u.s.', 'gen.', 'a.m.', 'p.', 'n.', 'a.', 'mr.', 'jr.', 'u.s.s.r.', 'mrs.', 'm.', 'frightened.to', 'v.', 'u.n.', '...', 'st.', 'oct.', 'col.', 'messrs.', 'camps.if', 'i.', 's.', 'p.m.', 'ph.d.', '..', 'u.', 'e.', 'dr.', 'co.', 't.', '.', 'f.', 'w.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'ll\", \"'s\", \"'ve\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"messrs.\", \"gen.\", \"u.n.\", \"a.m.\", \"st.\", \"u.s.\", \"ph.d\", \"jr.\", \"p.m.\", \"mrs.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'ll\", \"'s\", \"'ve\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 'messrs.', 'gen.', 'u.n.', 'a.m.', 'st.', 'u.s.', 'ph.d', 'jr.', 'p.m.', 'mrs.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"'mvd\", \"'madam\", \"'are\", \"'structure\", \"'democracy\", \"'into\", \"n't\", \"'liberty\", \"'\", \"'m\", \"o'clock\", \"'d\", \"'heat\", \"'system\", \"'german\", \"'blamed\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1_ = symbol.remove_apostrophe(data=tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'p.', 'n.', 'a.', 'm.', 'frightened.to', 'v.', '...', 'oct.', 'col.', 'camps.if', 'i.', 's.', 'ph.d.', '..', 'u.', 'e.', 'co.', 't.', '.', 'f.', 'w.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1__ = symbol.remove_dot(data=tokenized_1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 token 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'v', 'i', 'x', 'r', 'd', 'g', 'o', 'y', 'k', 's', 'b', 'u', 'w', 'h', 'e', 'm', 't', 'n', 'a', 'f', 'j', 'p'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_1__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 Token들 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'v', 'i', 'x', 'r', 'd', 'g', 'o', 'y', 'k', 's', 'b', 'u', 'w', 'h', 'e', 'm', 't', 'n', 'a', 'f', 'j', 'p'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1___ = symbol.remove_invalid_tokens(data=tokenized_1__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 invalid한 token이 있는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_1___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peiod 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(tokenized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'for\", \"n't\", \"'\", \"'m\", \"'reprisals\", \"'s\"}\n",
      "dot을 가진 token : \n",
      "{'gen.', 'p.', 'n.', 'a.', 'b.', 'mr.', 'u.s.s.r.', 'm.', 'v.', 'g.', 'r.', 'messrs.', 'i.', 's.', 'o.', 'l.', 'p.m.', '..', 'u.s.a.', 'c.', 'h.', 'e.', 'dr.', 't.', '.', 'tyranny.the', 'w.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'s\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"messrs.\", \"gen.\", \"u.s.a.\", \"p.m.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'s\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 'messrs.', 'gen.', 'u.s.a.', 'p.m.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"'for\", \"n't\", \"'\", \"'m\", \"'reprisals\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2_ = symbol.remove_apostrophe(data=tokenized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'p.', 'n.', 'a.', 'b.', 'm.', 'v.', 'g.', 'r.', 'i.', 's.', 'o.', 'l.', '..', 'c.', 'h.', 'e.', 't.', '.', 'tyranny.the', 'w.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2__ = symbol.remove_dot(data=tokenized_2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 Token들 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'v', 'i', 'l', 'r', 'd', 'g', 'o', 's', 'b', 'w', 'h', 'c', 'e', 'm', 't', 'n', 'a', 'f', 'p'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_2__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 token 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'i', 'v', 'l', 'r', 'd', 'g', 'o', 's', 'b', 'w', 'h', 'c', 'e', 'm', 't', 'n', 'a', 'f', 'p'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2___ = symbol.remove_invalid_tokens(data=tokenized_2__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 Invalid한 Token이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_2___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(tokenized_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'vas\", \"'ve\", \"'ll\", \"'has\", \"'spontaneous\", \"n't\", \"'\", \"o'clock\", \"'d\", \"'recession\", \"'s\"}\n",
      "dot을 가진 token : \n",
      "{'gen.', 'a.m.', 'p.', 'n.', 'j.', 'a.', 'b.', 'mr.', 'u.s.s.r.', 'mrs.', 'jr.', 'm.', 'v.', 'g.', '...', 'd.', '..................', 'st.', 'prof.', 'r.', 'u.n.r.r.a', 'col.', 'i.', 's.s.r', 's.', 'o.', 'l.', 'p.m.', 'u.', 'c.', 'h.', 'e.', 'dr.', 'maj.', 't.', '.', 'f.', 'w.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'ll\", \"'s\", \"'ve\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"s.s.r\", \"a.m.\", \"st.\", \"prof.\", \"u.n.r.r.a\", \"jr.\", \"maj.\", \"p.m.\", \"mrs.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'ll\", \"'s\", \"'ve\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 's.s.r', 'a.m.', 'st.', 'prof.', 'u.n.r.r.a', 'jr.', 'maj.', 'p.m.', 'mrs.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"'vas\", '``', \"'has\", \"'spontaneous\", \"n't\", \"'\", \"o'clock\", \"'d\", \"'recession\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3_ = symbol.remove_apostrophe(tokenized_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'gen.', 'p.', 'n.', 'j.', 'a.', 'b.', 'm.', 'v.', 'g.', '...', 'd.', '..................', 'r.', 'col.', 'i.', 's.', 'o.', 'l.', 'u.', 'c.', 'h.', 'e.', 't.', '.', 'f.', 'w.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3__ = symbol.remove_dot(tokenized_3_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 token 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'v', 'i', 'x', 'l', 'r', 'd', 'g', 'o', 's', 'b', 'u', 'w', 'h', 'c', 'e', 'm', 't', 'n', 'a', 'f', 'j', 'p'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(tokenized_3__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 token 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'i', 'v', 'x', 'l', 'r', 'd', 'g', 'o', 's', 'b', 'u', 'w', 'h', 'c', 'e', 'm', 't', 'n', 'a', 'f', 'j', 'p'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3___ = symbol.remove_invalid_tokens(tokenized_3__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 INvalid한 token이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(tokenized_3___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['would', 'could', 'might', 'need', 'can', 'must', \\\n",
    "    'one', 'two', 'upon', 'may', 'perhaps', 'living', 'seem', 'also', 'ii', 'ofthe',\n",
    "    'also', 'much', 'therefore']\n",
    "\n",
    "wo_stopword_1 = removeStopWords_ST(tokenized_1___, stopwords, new_stopwords)\n",
    "wo_stopword_2 = removeStopWords_ST(tokenized_2___, stopwords, new_stopwords)\n",
    "wo_stopword_3 = removeStopWords_ST(tokenized_3___, stopwords, new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(data, lemmatizer):\n",
    "    result = []\n",
    "\n",
    "    for article in data:\n",
    "        arr = []\n",
    "        for token in article:\n",
    "            if \".\" in token : arr.append(token)\n",
    "            elif \"'\" in token : arr.append(token)\n",
    "            elif token == \"us\" : arr.append(token)\n",
    "            else : \n",
    "                lemmatized = lemmatizer.lemmatize(token)\n",
    "                # if len(lemmatized) == 1: print(f\"token : {token} | lemmatized : {lemmatized}\")\n",
    "                arr.append(lemmatized)\n",
    "        result.append(arr)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_1 = lemmatizing(wo_stopword_1, lemmatizer)\n",
    "lemmatized_2 = lemmatizing(wo_stopword_2, lemmatizer)\n",
    "lemmatized_3 = lemmatizing(wo_stopword_3, lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagList = [['noun', ['NN','NNS','NNP','NNPS'], ['NNG','NNB','NNP','NNM']], \\\n",
    "    ['pronoun', ['PRP','WP','PRP'], ['NP']],\n",
    "    ['verb', ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'], ['VV', 'VXV', 'VCP']],\n",
    "    ['adjective', ['JJ', 'JJR', 'JJS'], ['VA', 'VXA', 'VCN']],\n",
    "    ['adverb', ['RB', 'RBR', 'RBS', 'WRB', 'EX', 'RP'], ['MAG']],\n",
    "    ['prep&conj', ['TO', 'IN', 'CC'], ['MAC']],\n",
    "    ['determiner', ['DT', 'PDT', 'WDT'], ['MDT', 'MDN']],\n",
    "    ['interjection',['UH'], ['IC']],\n",
    "    ['number', ['CD'], ['NR', 'ON']],\n",
    "    ['foreignW', ['FW'],['OL']],\n",
    "    ['modal',['MD'],[]],\n",
    "    ['josa', [], ['JC', 'JK', 'JKC', 'JKG', 'JKI', 'JKM', 'JKO', 'JKQ', 'JKS', 'JX']],\n",
    "    ['possesiveS', ['POS'], []],\n",
    "    ['others',['LS'], ['EPH', 'EPT', 'EPP', 'EFN', 'EFQ', 'EFO', 'EFA', 'EFI', 'EFR', 'ECE', 'ECD', 'ECS', 'ETN', 'ETD', 'XPN', 'XPV', 'XSN', 'XSV', 'XSA', 'XR', 'UN', 'OH']]]\n",
    "\n",
    "tagList = pd.DataFrame(tagList)\n",
    "tagList.columns = ['POS', 'Eng_tag', 'Kor_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344eb253d6894d19b798e315d6918b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d709c8b6dc4fa39ce767e43054301d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f88654b0d5c4ba3b6e5b630e571cdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged_1 = tagging(lemmatized_1, mode='ST')\n",
    "tagged_2 = tagging(lemmatized_2, mode='ST')\n",
    "tagged_3 = tagging(lemmatized_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POS 분류 결과 검증\n",
    "\n",
    "[검증 내용]\n",
    "1. dot을 포함한 token이 올바르게 분류되었는가?\n",
    "2. apostropphe를 포함한 token이 올바르게 분류되었는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class verify_tag:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def token_with_symbol(self):\n",
    "        dots = []\n",
    "        apostrophes = []\n",
    "\n",
    "        for tags in self.data:\n",
    "            for tag in tags:\n",
    "                if \".\" in tag[0] : dots.append(tag)\n",
    "                elif \"'\" in tag[0] : apostrophes.append(tag)\n",
    "        \n",
    "        self.apostrophes = set(apostrophes)\n",
    "        self.dots = set(dots)\n",
    "\n",
    "        print(f\"tagged token include apostrophe : \\n{set(apostrophes)}\")\n",
    "        print(f\"tagged token include dot : \\n{set(dots)}\") \n",
    "\n",
    "    def verify_dot_token(self):\n",
    "        for tag in self.dots:\n",
    "            removed = nltk.pos_tag([re.sub(\"[^a-z]\", \"\", tag[0])])\n",
    "            print(f\"{tag[0]} -> {tag[1]}   |   {removed[0][0]} -> {removed[0][1]}\")\n",
    "    \n",
    "    def verify_apostrophe_token(self):\n",
    "        for tag in self.apostrophes:\n",
    "            removed = nltk.pos_tag([re.sub(\"[^a-z]\", \"\", tag[0])])\n",
    "            print(f\"{tag[0]} -> {tag[1]}   |   {removed[0][0]} -> {removed[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify = verify_tag(data=tagged_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### symbol을 가진 token들을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token include apostrophe : \n",
      "{(\"'ve\", 'VBP'), (\"'s\", 'POS'), (\"'ll\", 'MD')}\n",
      "tagged token include dot : \n",
      "{('mr.', 'VBZ'), ('u.n.', 'NN'), ('a.m.', 'JJ'), ('gen.', 'NN'), ('mrs.', 'NNS'), ('u.s.', 'JJ'), ('st.', 'NN'), ('mr.', 'JJ'), ('jr.', 'VBP'), ('gen.', 'JJ'), ('messrs.', 'NN'), ('jr.', 'NN'), ('dr.', 'NN'), ('mr.', 'NNP'), ('st.', 'JJ'), ('mr.', 'VBP'), ('u.s.s.r.', 'JJ'), ('dr.', 'VB'), ('p.m.', 'RB'), ('mr.', 'NN')}\n"
     ]
    }
   ],
   "source": [
    "verify.token_with_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot을 가진 token 검증\n",
    "\n",
    "- dot을 넣고 빼서 Pos tagging을 수행하고 각각의 결과가 올바른지 결정.\n",
    "- gen. -> Gulag is headed by Gen. Victor nedosyekin, first vice minister of internal affairs, 에서 나옴.\n",
    "    - 사람 이름인 듯 하다.\n",
    "- **messrs., mr., dr., gen., jr., st., mrs., a.m., p.m.**\n",
    "    - '약어'로 tagging이 되어야 할 듯 하다.\n",
    "- **u.s.s.r., u.s., u.n.**\n",
    "    - '명사'로 tagging이 되어야 할 듯 하다.\n",
    "- 수정 방안\n",
    "    - 각각의 올바른 tag로 변경 해주기\n",
    "    - 확실한 구분을 위해 dot(.)은 유지시킨 채로 각 token들의 tag 바꾸어 주는 방향으로 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. -> VBZ   |   mr -> NN\n",
      "u.n. -> NN   |   un -> NN\n",
      "a.m. -> JJ   |   am -> VBP\n",
      "gen. -> NN   |   gen -> NN\n",
      "mrs. -> NNS   |   mrs -> NN\n",
      "u.s. -> JJ   |   us -> PRP\n",
      "st. -> NN   |   st -> NN\n",
      "mr. -> JJ   |   mr -> NN\n",
      "jr. -> VBP   |   jr -> NN\n",
      "gen. -> JJ   |   gen -> NN\n",
      "messrs. -> NN   |   messrs -> NN\n",
      "jr. -> NN   |   jr -> NN\n",
      "dr. -> NN   |   dr -> NN\n",
      "mr. -> NNP   |   mr -> NN\n",
      "st. -> JJ   |   st -> NN\n",
      "mr. -> VBP   |   mr -> NN\n",
      "u.s.s.r. -> JJ   |   ussr -> NN\n",
      "dr. -> VB   |   dr -> NN\n",
      "p.m. -> RB   |   pm -> NN\n",
      "mr. -> NN   |   mr -> NN\n"
     ]
    }
   ],
   "source": [
    "verify.verify_dot_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe를 가진 token 처리\n",
    "\n",
    "- apostrophe(')를 포함한 채로 tagging 하는 것이 더 정확하게 동작하는 것을 확인하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ve -> VBP   |   ve -> NN\n",
      "'s -> POS   |   s -> NN\n",
      "'ll -> MD   |   ll -> NN\n"
     ]
    }
   ],
   "source": [
    "verify.verify_apostrophe_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify = verify_tag(data=tagged_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### symbol을 가진 token들 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token include apostrophe : \n",
      "{(\"'s\", 'POS')}\n",
      "tagged token include dot : \n",
      "{('mr.', 'NNP'), ('gen.', 'JJ'), ('mr.', 'VBZ'), ('messrs.', 'NN'), ('mr.', 'FW'), ('p.m.', 'JJ'), ('dr.', 'NN'), ('mr.', 'VBP'), ('mr.', 'JJ'), ('u.s.s.r.', 'JJ'), ('mr.', 'VBD'), ('mr.', 'NNS'), ('mr.', 'RBS'), ('mr.', 'VB'), ('mr.', 'NN'), ('messrs.', 'NNS'), ('u.s.a.', 'NN')}\n"
     ]
    }
   ],
   "source": [
    "verify.token_with_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot을 가진 token 검증\n",
    "\n",
    "- dot을 넣고 빼서 Pos tagging을 수행하고 각각의 결과가 올바른지 결정.\n",
    "- gen. -> Gulag is headed by Gen. Victor nedosyekin, first vice minister of internal affairs, 에서 나옴.\n",
    "    - 사람 이름인 듯 하다.\n",
    "- **messrs., mr., dr., gen., jr., st., mrs., a.m., p.m.**\n",
    "    - '약어'로 tagging이 되어야 할 듯 하다.\n",
    "- **u.s.s.r., u.s.a., u.n.**\n",
    "    - '명사'로 tagging이 되어야 할 듯 하다.\n",
    "- 수정 방안\n",
    "    - 각각의 올바른 tag로 변경 해주기\n",
    "    - 확실한 구분을 위해 dot(.)은 유지시킨 채로 각 token들의 tag 바꾸어 주는 방향으로 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. -> NNP   |   mr -> NN\n",
      "gen. -> JJ   |   gen -> NN\n",
      "mr. -> VBZ   |   mr -> NN\n",
      "messrs. -> NN   |   messrs -> NN\n",
      "mr. -> FW   |   mr -> NN\n",
      "p.m. -> JJ   |   pm -> NN\n",
      "dr. -> NN   |   dr -> NN\n",
      "mr. -> VBP   |   mr -> NN\n",
      "mr. -> JJ   |   mr -> NN\n",
      "u.s.s.r. -> JJ   |   ussr -> NN\n",
      "mr. -> VBD   |   mr -> NN\n",
      "mr. -> NNS   |   mr -> NN\n",
      "mr. -> RBS   |   mr -> NN\n",
      "mr. -> VB   |   mr -> NN\n",
      "mr. -> NN   |   mr -> NN\n",
      "messrs. -> NNS   |   messrs -> NN\n",
      "u.s.a. -> NN   |   usa -> NN\n"
     ]
    }
   ],
   "source": [
    "verify.verify_dot_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe를 가진 token 처리\n",
    "\n",
    "- apostrophe(')를 포함한 채로 tagging 하는 것이 더 정확하게 동작하는 것을 확인하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s -> POS   |   s -> NN\n"
     ]
    }
   ],
   "source": [
    "verify.verify_apostrophe_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify = verify_tag(data=tagged_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### symbol을 가진 token들을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token include apostrophe : \n",
      "{(\"'ve\", 'VBP'), (\"'s\", 'POS'), (\"'ll\", 'MD')}\n",
      "tagged token include dot : \n",
      "{('mr.', 'VBZ'), ('mr.', 'FW'), ('dr.', 'JJ'), ('a.m.', 'VBD'), ('mr.', 'VB'), ('a.m.', 'RB'), ('u.n.r.r.a', 'JJ'), ('mrs.', 'NN'), ('p.m.', 'NN'), ('prof.', 'NN'), ('u.n.r.r.a', 'RB'), ('mr.', 'JJ'), ('mr.', 'VBD'), ('s.s.r', 'NN'), ('jr.', 'NN'), ('dr.', 'NN'), ('a.m.', 'NN'), ('dr.', 'VBZ'), ('mr.', 'NNP'), ('st.', 'JJ'), ('mr.', 'RBR'), ('maj.', 'NN'), ('u.s.s.r.', 'JJ'), ('mr.', 'NNS'), ('p.m.', 'RB'), ('mr.', 'NN')}\n"
     ]
    }
   ],
   "source": [
    "verify.token_with_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot을 가진 token 검증\n",
    "\n",
    "- dot을 넣고 빼서 Pos tagging을 수행하고 각각의 결과가 올바른지 결정.\n",
    "- gen. -> Gulag is headed by Gen. Victor nedosyekin, first vice minister of internal affairs, 에서 나옴.\n",
    "    - 사람 이름인 듯 하다.\n",
    "- **messrs., mr., dr., gen., maj., prof.,  jr., st., mrs., a.m., p.m.**\n",
    "    - '약어'로 tagging이 되어야 할 듯 하다.\n",
    "- **u.s.s.r., u.s.a., u.n., s.s.r, u.n.r.r.a**\n",
    "    - '명사'로 tagging이 되어야 할 듯 하다.\n",
    "- 수정 방안\n",
    "    - 각각의 올바른 tag로 변경 해주기\n",
    "    - 확실한 구분을 위해 dot(.)은 유지시킨 채로 각 token들의 tag 바꾸어 주는 방향으로 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. -> VBZ   |   mr -> NN\n",
      "mr. -> FW   |   mr -> NN\n",
      "dr. -> JJ   |   dr -> NN\n",
      "a.m. -> VBD   |   am -> VBP\n",
      "mr. -> VB   |   mr -> NN\n",
      "a.m. -> RB   |   am -> VBP\n",
      "u.n.r.r.a -> JJ   |   unrra -> NN\n",
      "mrs. -> NN   |   mrs -> NN\n",
      "p.m. -> NN   |   pm -> NN\n",
      "prof. -> NN   |   prof -> NN\n",
      "u.n.r.r.a -> RB   |   unrra -> NN\n",
      "mr. -> JJ   |   mr -> NN\n",
      "mr. -> VBD   |   mr -> NN\n",
      "s.s.r -> NN   |   ssr -> NN\n",
      "jr. -> NN   |   jr -> NN\n",
      "dr. -> NN   |   dr -> NN\n",
      "a.m. -> NN   |   am -> VBP\n",
      "dr. -> VBZ   |   dr -> NN\n",
      "mr. -> NNP   |   mr -> NN\n",
      "st. -> JJ   |   st -> NN\n",
      "mr. -> RBR   |   mr -> NN\n",
      "maj. -> NN   |   maj -> NN\n",
      "u.s.s.r. -> JJ   |   ussr -> NN\n",
      "mr. -> NNS   |   mr -> NN\n",
      "p.m. -> RB   |   pm -> NN\n",
      "mr. -> NN   |   mr -> NN\n"
     ]
    }
   ],
   "source": [
    "verify.verify_dot_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe를 가진 token 처리\n",
    "\n",
    "- apostrophe(')를 포함한 채로 tagging 하는 것이 더 정확하게 동작하는 것을 확인하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ve -> VBP   |   ve -> NN\n",
      "'s -> POS   |   s -> NN\n",
      "'ll -> MD   |   ll -> NN\n"
     ]
    }
   ],
   "source": [
    "verify.verify_apostrophe_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adress POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_token_to_noun(data):\n",
    "    result = []\n",
    "\n",
    "    for tags in data:\n",
    "        arr = []\n",
    "        for tag in tags:\n",
    "            if \".\" in tag[0] : arr.append((tag[0], \"NN\"))\n",
    "            else : arr.append(tag)\n",
    "        result.append(arr)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_1_ = dot_token_to_noun(data=tagged_1)\n",
    "tagged_2_ = dot_token_to_noun(data=tagged_2)\n",
    "tagged_3_ = dot_token_to_noun(data=tagged_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save PreProcessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_ROOT = './Preprocessed/'\n",
    "\n",
    "SAVE_1 = SAVE_ROOT + 'period-1/'\n",
    "SAVE_2 = SAVE_ROOT + 'period-2/'\n",
    "SAVE_3 = SAVE_ROOT + 'period-3/'\n",
    "\n",
    "def to_pickle(data, file_name, root='./'):\n",
    "    with open(root+file_name+'.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatized data to pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1, file_name=\"lemmatized-all\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2, file_name=\"lemmatized-all\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3, file_name=\"lemmatized-all\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=extract_some_pos_ST(tagged_1_, tagList=tagList, pos_list=['noun']), \\\n",
    "    file_name=\"lemmatized-noun\", root=SAVE_1)\n",
    "to_pickle(data=extract_some_pos_ST(tagged_2_, tagList=tagList, pos_list=['noun']), \\\n",
    "    file_name=\"lemmatized-noun\", root=SAVE_2)\n",
    "to_pickle(data=extract_some_pos_ST(tagged_3_, tagList=tagList, pos_list=['noun']), \\\n",
    "    file_name=\"lemmatized-noun\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=extract_some_pos_ST(tagged_1_, tagList=tagList, pos_list=['verb']), \\\n",
    "    file_name=\"lemmatized-verb\", root=SAVE_1)\n",
    "to_pickle(data=extract_some_pos_ST(tagged_2_, tagList=tagList, pos_list=['verb']), \\\n",
    "    file_name=\"lemmatized-verb\", root=SAVE_2)\n",
    "to_pickle(data=extract_some_pos_ST(tagged_3_, tagList=tagList, pos_list=['verb']), \\\n",
    "    file_name=\"lemmatized-verb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=extract_some_pos_ST(tagged_1_, tagList=tagList, pos_list=['adjective']), \\\n",
    "    file_name=\"lemmatized-adjective\", root=SAVE_1)\n",
    "to_pickle(data=extract_some_pos_ST(tagged_2_, tagList=tagList, pos_list=['adjective']), \\\n",
    "    file_name=\"lemmatized-adjective\", root=SAVE_2)\n",
    "to_pickle(data=extract_some_pos_ST(tagged_3_, tagList=tagList, pos_list=['adjective']), \\\n",
    "    file_name=\"lemmatized-adjective\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=extract_some_pos_ST(tagged_1_, tagList=tagList, pos_list=['adverb']), \\\n",
    "    file_name=\"lemmatized-adverb\", root=SAVE_1)\n",
    "to_pickle(data=extract_some_pos_ST(tagged_2_, tagList=tagList, pos_list=['adverb']), \\\n",
    "    file_name=\"lemmatized-adverb\", root=SAVE_2)\n",
    "to_pickle(data=extract_some_pos_ST(tagged_3_, tagList=tagList, pos_list=['adverb']), \\\n",
    "    file_name=\"lemmatized-adverb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagged data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=tagged_1_, file_name=\"tagged-all\", root=SAVE_1)\n",
    "to_pickle(data=tagged_2_, file_name=\"tagged-all\", root=SAVE_2)\n",
    "to_pickle(data=tagged_3_, file_name=\"tagged-all\", root=SAVE_3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a3111211be4281f3a8c4a9b25563b8d253df502c7e31f5318895c1792a97cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
