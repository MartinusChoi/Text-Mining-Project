{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Data PreProcessing\n",
    "\n",
    "## [주요 고려 사항]\n",
    "1. `dot(.)`과 `apostrophe(')` 처리\n",
    "    - 'u.s.'와 'u.s.s.r.'과 같은 약자처리를 어떻게 할 것인가?\n",
    "    - 'america's'와 같은 소유격을 어떻게 처리할 것인가?\n",
    "        1. 처음 Cleaning 때, `dot(.)`과 `apostrophe(')`는 제거하지 않음\n",
    "            - `dot(.)`\n",
    "                - 'u.s', 'u.s.s.r'과 같은 약자를 유지시키기 위한 처리\n",
    "            - `apostrophe(')`\n",
    "                - 'america's'와 같은 소유격을 유지시켜서 Tokenizing때 's를 분리시키기 위함.\n",
    "        2. Tokenizing 이후, `dot(.)`과 `apostrophe(')`를 유지시켜야 하는 Token들 외에는 특수문자 제거\n",
    "            1. `apostrophe(')`와 `dot(.)`을 가진 Token들을 출력해보고 유지시킬 Token들의 목록을 결정\n",
    "            2. `apostrophe(')`를 유지시킬 Token들 외의 모든 Token들에서 `apostrophe(')` 및 특수문자 제거\n",
    "                - `dot(.)`은 다음 단계에서 예외처리를 하며 제거해야 하므로, 이 단계에서는 모든 `dot(.)`을 유지시킴\n",
    "            3. `dot(.)`을 유지시킬 Token들 외의 모든 Token들에서 `dot(.)` 및 특수문자 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined Modules\n",
    "from myModules.utils import DataLoader, merge\n",
    "from myModules.preprocess.english import cleaning, remove_stopwords, tagging, dot_and_apostrophe, convert_pos, lemmatization, to_pickle, check_pos\n",
    "\n",
    "# General Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read File\n",
    "import glob\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = './Data/3구간/'\n",
    "\n",
    "PERIOD_1 = DATA_ROOT + '1시기/1시기_ST/'\n",
    "PERIOD_2 = DATA_ROOT + '2시기/2시기_ST/'\n",
    "PERIOD_3 = DATA_ROOT + '3시기/3시기_ST/'\n",
    "\n",
    "RESULT_ROOT = './Result/3구간/'\n",
    "\n",
    "RESULT_1 = RESULT_ROOT + '/1시기/ST/'\n",
    "RESULT_2 = RESULT_ROOT + '/2시기/ST/'\n",
    "RESULT_3 = RESULT_ROOT + '/3시기/ST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_1 = glob.glob(PERIOD_1+'*.txt')\n",
    "files_list_2 = glob.glob(PERIOD_2+'*.txt')\n",
    "files_list_3 = glob.glob(PERIOD_3+'*.txt')\n",
    "\n",
    "texts_1 = DataLoader(files_list_1, mode='ST')\n",
    "texts_2 = DataLoader(files_list_2, mode='ST')\n",
    "texts_3 = DataLoader(files_list_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Data Cleaning\n",
    "\n",
    "- `dot(.)`과 `apostrophe(')`는 제거하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_1 = cleaning(data=texts_1)\n",
    "cleaned_2 = cleaning(data=texts_2)\n",
    "cleaned_3 = cleaning(data=texts_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_1 = [word_tokenize(text) for text in cleaned_1]\n",
    "tokenized_2 = [word_tokenize(text) for text in cleaned_2]\n",
    "tokenized_3 = [word_tokenize(text) for text in cleaned_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(data=tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'ve\", \"o'clock\", \"'madam\", \"n't\", \"'m\", \"'are\", \"'liberty\", \"'ll\", \"'german\", \"'\", \"'system\", \"'s\", \"'heat\", \"'d\", \"'democracy\", \"'blamed\", \"'into\", \"'mvd\", \"'structure\"}\n",
      "dot을 가진 token : \n",
      "{'co.', 'a.m.', 'i.', 'm.', 'ph.d.', 'n.', 'col.', 't.', 'u.s.s.r.', 'jr.', 'frightened.to', 'u.s.', 'dr.', '...', 'mrs.', 'p.m.', '.', 'mr.', 'st.', 'u.n.', 'f.', 's.', 'u.', 'a.', 'e.', '..', 'camps.if', 'oct.', 'w.', 'gen.', 'p.', 'messrs.', 'v.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'ll\", \"'s\", \"'ve\", \"n't\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"messrs.\", \"gen.\", \"u.n.\", \"a.m.\", \"st.\", \"u.s.\", \"ph.d\", \"jr.\", \"p.m.\", \"mrs.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'ll\", \"'s\", \"'ve\", \"n't\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 'messrs.', 'gen.', 'u.n.', 'a.m.', 'st.', 'u.s.', 'ph.d', 'jr.', 'p.m.', 'mrs.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"'are\", \"'d\", \"'liberty\", \"'democracy\", \"o'clock\", \"'mvd\", \"'blamed\", \"'german\", \"'\", \"'madam\", \"'system\", \"'into\", \"'structure\", \"'m\", \"'heat\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1_ = symbol.remove_apostrophe(data=tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'co.', 'i.', 'm.', 'ph.d.', 'col.', 'n.', 't.', 'frightened.to', '...', '.', 'f.', 's.', 'u.', 'a.', 'e.', '..', 'camps.if', 'oct.', 'w.', 'p.', 'v.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1__ = symbol.remove_dot(data=tokenized_1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 token 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'i', 'g', 'e', 'p', 'f', 'x', 'h', 'm', 'b', 's', 'a', 'v', 'n', 'w', 'd', 'j', 'k', 'o', 't', 'y', 'r', 'u'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_1__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 Token들 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'i', 'g', 'e', 'p', 'f', 'x', 'h', 'm', 'b', 's', 'a', 'v', 'n', 'w', 'd', 'j', 'k', 'o', 't', 'y', 'r', 'u'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1___ = symbol.remove_invalid_tokens(data=tokenized_1__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 invalid한 token이 있는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_1___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peiod 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(data=tokenized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'reprisals\", \"'\", \"'for\", \"n't\", \"'m\", \"'s\"}\n",
      "dot을 가진 token : \n",
      "{'r.', 'tyranny.the', 'u.s.a.', 'i.', 'm.', 'n.', 't.', 'u.s.s.r.', 'g.', 'b.', 'l.', 'dr.', 'h.', 'p.m.', '.', 'mr.', 'o.', 's.', 'a.', 'e.', '..', 'w.', 'gen.', 'p.', 'c.', 'messrs.', 'v.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'s\", \"n't\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"messrs.\", \"gen.\", \"u.s.a.\", \"p.m.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'s\", \"n't\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 'messrs.', 'gen.', 'u.s.a.', 'p.m.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"'for\", \"'reprisals\", \"'\", \"'m\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2_ = symbol.remove_apostrophe(data=tokenized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'r.', 'tyranny.the', 'i.', 'm.', 'n.', 't.', 'g.', 'b.', 'l.', 'h.', '.', 'o.', 's.', 'a.', 'e.', '..', 'w.', 'p.', 'c.', 'v.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2__ = symbol.remove_dot(data=tokenized_2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 Token들 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'i', 'g', 'e', 'p', 'f', 'c', 'h', 'm', 'b', 's', 'a', 'v', 'n', 'w', 'd', 'o', 'l', 't', 'r'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_2__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 token 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'i', 'g', 'e', 'p', 'f', 'c', 'h', 'm', 'b', 's', 'a', 'v', 'n', 'w', 'd', 'o', 'l', 't', 'r'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2___ = symbol.remove_invalid_tokens(data=tokenized_2__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 Invalid한 Token이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_2___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(tokenized_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'ve\", \"'has\", \"'d\", \"'vas\", \"o'clock\", \"'spontaneous\", \"'recession\", \"'ll\", \"'\", \"n't\", \"'s\"}\n",
      "dot을 가진 token : \n",
      "{'r.', 's.s.r', 'a.m.', 'i.', 'm.', 'col.', 'n.', 't.', '..................', 'g.', 'u.s.s.r.', 'jr.', 'b.', 'l.', 'dr.', '...', 'prof.', 'j.', 'mrs.', 'h.', 'maj.', 'p.m.', '.', 'mr.', 'o.', 'st.', 'd.', 'f.', 's.', 'u.', 'a.', 'u.n.r.r.a', 'e.', 'w.', 'gen.', 'p.', 'c.', 'v.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'ll\", \"'s\", \"'ve\", \"n't\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"s.s.r\", \"a.m.\", \"st.\", \"prof.\", \"u.n.r.r.a\", \"jr.\", \"maj.\", \"p.m.\", \"mrs.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'ll\", \"'s\", \"'ve\", \"n't\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 's.s.r', 'a.m.', 'st.', 'prof.', 'u.n.r.r.a', 'jr.', 'maj.', 'p.m.', 'mrs.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'``', \"'has\", \"'d\", \"'vas\", \"o'clock\", \"'spontaneous\", \"'recession\", \"'\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3_ = symbol.remove_apostrophe(tokenized_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'r.', 'i.', 'm.', 'n.', 'col.', 't.', '..................', 'g.', 'b.', 'l.', '...', 'j.', 'h.', '.', 'o.', 'd.', 'f.', 's.', 'u.', 'a.', 'e.', 'w.', 'gen.', 'p.', 'c.', 'v.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3__ = symbol.remove_dot(tokenized_3_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 token 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'i', 'g', 'e', 'p', 'f', 'x', 'c', 'h', 'm', 'b', 's', 'a', 'v', 'n', 'w', 'd', 'j', 'o', 'l', 't', 'r', 'u'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(tokenized_3__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 token 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'i', 'g', 'e', 'p', 'f', 'x', 'c', 'h', 'm', 'b', 's', 'a', 'v', 'n', 'w', 'd', 'j', 'o', 'l', 't', 'r', 'u'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3___ = symbol.remove_invalid_tokens(tokenized_3__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 INvalid한 token이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(tokenized_3___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['would', 'could', 'might', 'need', 'can', 'must', \\\n",
    "    'one', 'two', 'upon', 'may', 'perhaps', 'living', 'seem', 'also', 'ii', 'ofthe',\n",
    "    'also', 'much', 'therefore', \"'ll\", \"'ve\", \"n't\"]\n",
    "\n",
    "wo_stopword_1 = remove_stopwords(tokenized_1___, stopwords, new_stopwords)\n",
    "wo_stopword_2 = remove_stopwords(tokenized_2___, stopwords, new_stopwords)\n",
    "wo_stopword_3 = remove_stopwords(tokenized_3___, stopwords, new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_table = pd.read_pickle(\"processed-data/pos-table.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcfe992d0ee440687d5128336951db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203da72d51904d78b26d583d967a3aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2409d65896c7411ba07be563f90a76cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged_1 = tagging(wo_stopword_1)\n",
    "tagged_2 = tagging(wo_stopword_2)\n",
    "tagged_3 = tagging(wo_stopword_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'s\": {'POS'}}\n",
      "tagged token with dot : \n",
      "{'u.s.s.r.': {'JJ', 'VBP'}, 'jr.': {'NN', 'VBP'}, 'p.m.': {'RB'}, 'u.s.': {'JJ'}, 'mr.': {'JJ', 'RB', 'VBP', 'NNP', 'RBS', 'NN'}, 'gen.': {'JJ', 'NN', 'VBP'}, 'dr.': {'JJ', 'VBP'}, 'st.': {'JJ', 'NN'}, 'messrs.': {'NN'}, 'a.m.': {'JJ'}, 'u.n.': {'NN'}, 'mrs.': {'NNS'}}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'s': ['NN']}\n",
      "tagged token without dot : \n",
      "{'ussr': ['NN'], 'jr': ['NN'], 'pm': ['NN'], 'us': ['PRP'], 'mr': ['NN'], 'gen': ['NN'], 'dr': ['NN'], 'st': ['NN'], 'messrs': ['NN'], 'am': ['VBP'], 'un': ['NN'], 'mrs': ['NN']}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'s\": {'POS'}}\n",
      "tagged token with dot : \n",
      "{'u.s.s.r.': {'JJ'}, 'p.m.': {'JJ'}, 'mr.': {'JJ', 'FW', 'NNS', 'VBP', 'NNP', 'VB', 'RBS', 'NN', 'VBZ'}, 'u.s.a.': {'NN'}, 'gen.': {'JJ'}, 'dr.': {'NN'}, 'messrs.': {'NNS'}}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'s': ['NN']}\n",
      "tagged token without dot : \n",
      "{'ussr': ['NN'], 'pm': ['NN'], 'mr': ['NN'], 'usa': ['NN'], 'gen': ['NN'], 'dr': ['NN'], 'messrs': ['NN']}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'s\": {'POS'}}\n",
      "tagged token with dot : \n",
      "{'u.s.s.r.': {'JJ'}, 'jr.': {'NN'}, 'p.m.': {'RB', 'NN', 'VBP'}, 'mr.': {'JJ', 'RBR', 'FW', 'VBD', 'NNS', 'RB', 'VBP', 'NNP', 'VB', 'RBS', 'NN', 'VBZ'}, 's.s.r': {'NN'}, 'dr.': {'JJ', 'NN', 'VBZ', 'VBP'}, 'st.': {'JJ'}, 'maj.': {'NN'}, 'a.m.': {'RB', 'NN', 'VBD'}, 'prof.': {'NN'}, 'mrs.': {'NN'}, 'u.n.r.r.a': {'RB', 'JJ'}}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'s': ['NN']}\n",
      "tagged token without dot : \n",
      "{'ussr': ['NN'], 'jr': ['NN'], 'pm': ['NN'], 'mr': ['NN'], 'ssr': ['NN'], 'dr': ['NN'], 'st': ['NN'], 'maj': ['NN'], 'am': ['VBP'], 'prof': ['NN'], 'mrs': ['NN'], 'unrra': ['NN']}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. adress POS of token with symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_1_ = convert_pos(data=tagged_1, key=\".\", target_pos=\"NN\")\n",
    "tagged_2_ = convert_pos(data=tagged_2, key=\".\",  target_pos=\"NN\")\n",
    "tagged_3_ = convert_pos(data=tagged_3, key=\".\",  target_pos=\"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['noun', 'verb', 'adjective', 'adverb'])\n",
    "lemmatized_1_all = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['noun', 'verb', 'adjective', 'adverb'])\n",
    "lemmatized_2_all = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['noun', 'verb', 'adjective', 'adverb'])\n",
    "lemmatized_3_all = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['noun'])\n",
    "lemmatized_1_noun = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['noun'])\n",
    "lemmatized_2_noun = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['noun'])\n",
    "lemmatized_3_noun = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['verb'])\n",
    "lemmatized_1_verb = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['verb'])\n",
    "lemmatized_2_verb = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['verb'])\n",
    "lemmatized_3_verb = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['adjective'])\n",
    "lemmatized_1_adjective = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['adjective'])\n",
    "lemmatized_2_adjective = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['adjective'])\n",
    "lemmatized_3_adjective = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['adverb'])\n",
    "lemmatized_1_adverb = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['adverb'])\n",
    "lemmatized_2_adverb = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['adverb'])\n",
    "lemmatized_3_adverb = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save PreProcessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_ROOT = './processed-data/'\n",
    "\n",
    "SAVE_1 = SAVE_ROOT + 'period-1/'\n",
    "SAVE_2 = SAVE_ROOT + 'period-2/'\n",
    "SAVE_3 = SAVE_ROOT + 'period-3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. Preprocessed data to pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_all, file_name=\"lemmatized-all\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_all, file_name=\"lemmatized-all\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_all, file_name=\"lemmatized-all\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_noun, file_name=\"lemmatized-noun\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_noun, file_name=\"lemmatized-noun\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_noun, file_name=\"lemmatized-noun\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_verb, file_name=\"lemmatized-verb\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_verb, file_name=\"lemmatized-verb\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_verb, file_name=\"lemmatized-verb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_adjective, file_name=\"lemmatized-adjective\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_adjective, file_name=\"lemmatized-adjective\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_adjective, file_name=\"lemmatized-adjective\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_adverb, file_name=\"lemmatized-adverb\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_adverb, file_name=\"lemmatized-adverb\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_adverb, file_name=\"lemmatized-adverb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Tagged data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=tagged_1_, file_name=\"tagged\", root=SAVE_1)\n",
    "to_pickle(data=tagged_2_, file_name=\"tagged\", root=SAVE_2)\n",
    "to_pickle(data=tagged_3_, file_name=\"tagged\", root=SAVE_3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a3111211be4281f3a8c4a9b25563b8d253df502c7e31f5318895c1792a97cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
