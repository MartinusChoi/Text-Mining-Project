{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Data PreProcessing\n",
    "\n",
    "## [주요 고려 사항]\n",
    "1. `dot(.)`과 `apostrophe(')` 처리\n",
    "    - 'u.s.'와 'u.s.s.r.'과 같은 약자처리를 어떻게 할 것인가?\n",
    "    - 'america's'와 같은 소유격을 어떻게 처리할 것인가?\n",
    "        1. 처음 Cleaning 때, `dot(.)`과 `apostrophe(')`는 제거하지 않음\n",
    "            - `dot(.)`\n",
    "                - 'u.s', 'u.s.s.r'과 같은 약자를 유지시키기 위한 처리\n",
    "            - `apostrophe(')`\n",
    "                - 'america's'와 같은 소유격을 유지시켜서 Tokenizing때 's를 분리시키기 위함.\n",
    "        2. Tokenizing 이후, `dot(.)`과 `apostrophe(')`를 유지시켜야 하는 Token들 외에는 특수문자 제거\n",
    "            1. `apostrophe(')`와 `dot(.)`을 가진 Token들을 출력해보고 유지시킬 Token들의 목록을 결정\n",
    "            2. `apostrophe(')`를 유지시킬 Token들 외의 모든 Token들에서 `apostrophe(')` 및 특수문자 제거\n",
    "                - `dot(.)`은 다음 단계에서 예외처리를 하며 제거해야 하므로, 이 단계에서는 모든 `dot(.)`을 유지시킴\n",
    "            3. `dot(.)`을 유지시킬 Token들 외의 모든 Token들에서 `dot(.)` 및 특수문자 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined Modules\n",
    "from myModules.utils import DataLoader, merge\n",
    "from myModules.preprocess.english import cleaning, remove_stopwords, tagging, \\\n",
    "    dot_and_apostrophe, convert_pos, lemmatization_nltk, lemmatization_spacy, \\\n",
    "    to_pickle, to_csv, check_pos, pos_correction\n",
    "\n",
    "# General Modules\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read File\n",
    "import glob\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = './Data/3구간/'\n",
    "\n",
    "PERIOD_1 = DATA_ROOT + '1시기/1시기_ST/'\n",
    "PERIOD_2 = DATA_ROOT + '2시기/2시기_ST/'\n",
    "PERIOD_3 = DATA_ROOT + '3시기/3시기_ST/'\n",
    "\n",
    "RESULT_ROOT = './Result/3구간/'\n",
    "\n",
    "RESULT_1 = RESULT_ROOT + '/1시기/ST/'\n",
    "RESULT_2 = RESULT_ROOT + '/2시기/ST/'\n",
    "RESULT_3 = RESULT_ROOT + '/3시기/ST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_1 = glob.glob(PERIOD_1+'*.txt')\n",
    "files_list_2 = glob.glob(PERIOD_2+'*.txt')\n",
    "files_list_3 = glob.glob(PERIOD_3+'*.txt')\n",
    "\n",
    "texts_1 = DataLoader(files_list_1, mode='ST')\n",
    "texts_2 = DataLoader(files_list_2, mode='ST')\n",
    "texts_3 = DataLoader(files_list_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Data Cleaning\n",
    "\n",
    "- `dot(.)`과 `apostrophe(')`는 제거하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_1 = cleaning(data=texts_1)\n",
    "cleaned_2 = cleaning(data=texts_2)\n",
    "cleaned_3 = cleaning(data=texts_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_1 = [word_tokenize(text) for text in cleaned_1]\n",
    "tokenized_2 = [word_tokenize(text) for text in cleaned_2]\n",
    "tokenized_3 = [word_tokenize(text) for text in cleaned_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(data=tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'d\", \"'heat\", \"'ll\", \"'\", \"'mvd\", \"'m\", \"'s\", \"n't\", \"'into\", \"'democracy\", \"'structure\", \"'ve\", \"'are\", \"'liberty\", \"'madam\", \"o'clock\", \"'blamed\", \"'german\", \"'system\"}\n",
      "dot을 가진 token : \n",
      "{'u.s.s.r.', 'dr.', 'ph.d.', 'gen.', 'a.m.', 'mrs.', 'frightened.to', 'v.', 'col.', '...', 'm.', '..', 'a.', 'u.s.', 'st.', 'u.', 'camps.if', 'u.n.', 'jr.', 'w.', 'e.', 'p.', 'n.', 'messrs.', 't.', 'mr.', 'oct.', 'co.', 'i.', 's.', '.', 'p.m.', 'f.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'ll\", \"'s\", \"'ve\", \"n't\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"messrs.\", \"gen.\", \"u.n.\", \"a.m.\", \"st.\", \"u.s.\", \"ph.d\", \"jr.\", \"p.m.\", \"mrs.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'ll\", \"'s\", \"'ve\", \"n't\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 'messrs.', 'gen.', 'u.n.', 'a.m.', 'st.', 'u.s.', 'ph.d', 'jr.', 'p.m.', 'mrs.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"'d\", \"o'clock\", \"'blamed\", \"'german\", \"'heat\", \"'mvd\", \"'democracy\", \"'are\", \"'into\", \"'system\", \"'structure\", \"'liberty\", \"'\", \"'m\", \"'madam\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1_ = symbol.remove_apostrophe(data=tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'ph.d.', 'frightened.to', 'v.', 'col.', '...', 'm.', '..', 'a.', 'u.', 'camps.if', 'w.', 'e.', 'p.', 'n.', 't.', 'oct.', 'co.', 'i.', 's.', '.', 'f.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1__ = symbol.remove_dot(data=tokenized_1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 token 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'm', 'k', 't', 'o', 'u', 'a', 'y', 'j', 'h', 'r', 'b', 'g', 'p', 'f', 'w', 'e', 'd', 'n', 'v', 'x', 'i', 's'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_1__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 Token들 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'm', 'k', 't', 'o', 'u', 'a', 'y', 'j', 'h', 'r', 'b', 'g', 'p', 'f', 'w', 'e', 'd', 'n', 'v', 'x', 'i', 's'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1___ = symbol.remove_invalid_tokens(data=tokenized_1__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 invalid한 token이 있는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_1___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peiod 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(data=tokenized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"n't\", \"'reprisals\", \"'for\", \"'\", \"'m\", \"'s\"}\n",
      "dot을 가진 token : \n",
      "{'u.s.s.r.', 'dr.', 'gen.', 'v.', 'b.', 'm.', 'u.s.a.', '..', 'a.', 'h.', 'o.', 'w.', 'r.', 'e.', 'p.', 'n.', 'l.', 'g.', 'messrs.', 't.', 'tyranny.the', 'mr.', 'i.', 's.', '.', 'p.m.', 'c.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'s\", \"n't\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"messrs.\", \"gen.\", \"u.s.a.\", \"p.m.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'s\", \"n't\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 'messrs.', 'gen.', 'u.s.a.', 'p.m.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"'\", \"'reprisals\", \"'for\", \"'m\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2_ = symbol.remove_apostrophe(data=tokenized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'v.', 'b.', 'm.', '..', 'a.', 'h.', 'o.', 'w.', 'r.', 'e.', 'p.', 'n.', 'l.', 'g.', 't.', 'tyranny.the', 'i.', 's.', '.', 'c.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2__ = symbol.remove_dot(data=tokenized_2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 Token들 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'm', 't', 'c', 'o', 'a', 'r', 'h', 'b', 'g', 'p', 'f', 'w', 'e', 'd', 'n', 'v', 'l', 'i', 's'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_2__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 token 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'm', 't', 'c', 'o', 'a', 'r', 'h', 'b', 'g', 'p', 'f', 'w', 'e', 'd', 'n', 'v', 'l', 'i', 's'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2___ = symbol.remove_invalid_tokens(data=tokenized_2__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 Invalid한 Token이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_2___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(tokenized_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'d\", \"n't\", \"o'clock\", \"'spontaneous\", \"'recession\", \"'ve\", \"'vas\", \"'ll\", \"'\", \"'s\", \"'has\"}\n",
      "dot을 가진 token : \n",
      "{'dr.', 'u.s.s.r.', '..................', 'gen.', 'a.m.', 'mrs.', 'v.', 'j.', 'col.', 'b.', '...', 'm.', 'h.', 'a.', 'st.', 'o.', 'u.n.r.r.a', 'u.', 'jr.', 'prof.', 'r.', 'e.', 'p.', 'n.', 'l.', 'g.', 's.s.r', 'w.', 't.', 'mr.', 'i.', 's.', 'maj.', '.', 'p.m.', 'c.', 'd.', 'f.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'ll\", \"'s\", \"'ve\", \"n't\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"s.s.r\", \"a.m.\", \"st.\", \"prof.\", \"u.n.r.r.a\", \"jr.\", \"maj.\", \"p.m.\", \"mrs.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'ll\", \"'s\", \"'ve\", \"n't\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 's.s.r', 'a.m.', 'st.', 'prof.', 'u.n.r.r.a', 'jr.', 'maj.', 'p.m.', 'mrs.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"'d\", \"o'clock\", \"'spontaneous\", '``', \"'recession\", \"'vas\", \"'\", \"'has\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3_ = symbol.remove_apostrophe(tokenized_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', '..................', 'gen.', 'v.', 'col.', 'j.', 'b.', '...', 'm.', 'h.', 'a.', 'o.', 'u.', 'w.', 'r.', 'e.', 'p.', 'n.', 'l.', 'g.', 't.', 'i.', 's.', '.', 'c.', 'd.', 'f.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3__ = symbol.remove_dot(tokenized_3_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 token 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'm', 't', 'c', 'o', 'u', 'a', 'j', 'r', 'h', 'b', 'g', 'p', 'f', 'w', 'e', 'd', 'n', 'v', 'l', 'x', 'i', 's'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(tokenized_3__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 token 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'm', 't', 'c', 'o', 'u', 'a', 'j', 'r', 'h', 'b', 'g', 'p', 'f', 'w', 'e', 'd', 'n', 'v', 'l', 'x', 'i', 's'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3___ = symbol.remove_invalid_tokens(tokenized_3__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 INvalid한 token이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(tokenized_3___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['would', 'could', 'might', 'need', 'can', 'must', \\\n",
    "    'one', 'two', 'upon', 'may', 'perhaps', 'living', 'seem', 'also', 'ii', 'ofthe',\n",
    "    'also', 'much', 'therefore', \"'ll\", \"'ve\", \"n't\"]\n",
    "\n",
    "wo_stopword_1 = remove_stopwords(tokenized_1___, stopwords, new_stopwords)\n",
    "wo_stopword_2 = remove_stopwords(tokenized_2___, stopwords, new_stopwords)\n",
    "wo_stopword_3 = remove_stopwords(tokenized_3___, stopwords, new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_table = pd.read_pickle(\"processed-data/pos-table.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849fde78797346719081eaeaa7a42adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae742ebc7b744a4cb2b5cd76055f8435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196b5a17306f434896a44e48781d935b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged_1 = tagging(wo_stopword_1)\n",
    "tagged_2 = tagging(wo_stopword_2)\n",
    "tagged_3 = tagging(wo_stopword_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'s\": {'POS'}}\n",
      "tagged token with dot : \n",
      "{'jr.': {'NN', 'VBP'}, 'u.s.s.r.': {'VBP', 'JJ'}, 'dr.': {'VBP', 'JJ'}, 'gen.': {'NN', 'VBP', 'JJ'}, 'u.s.': {'JJ'}, 'st.': {'NN', 'JJ'}, 'a.m.': {'JJ'}, 'mrs.': {'NNS'}, 'messrs.': {'NN'}, 'p.m.': {'RB'}, 'mr.': {'NNP', 'RBS', 'JJ', 'NN', 'RB', 'VBP'}, 'u.n.': {'NN'}}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'s': ['NN']}\n",
      "tagged token without dot : \n",
      "{'jr': ['NN'], 'ussr': ['NN'], 'dr': ['NN'], 'gen': ['NN'], 'us': ['PRP'], 'st': ['NN'], 'am': ['VBP'], 'mrs': ['NN'], 'messrs': ['NN'], 'pm': ['NN'], 'mr': ['NN'], 'un': ['NN']}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'s\": {'POS'}}\n",
      "tagged token with dot : \n",
      "{'u.s.s.r.': {'JJ'}, 'dr.': {'NN'}, 'gen.': {'JJ'}, 'messrs.': {'NNS'}, 'p.m.': {'JJ'}, 'mr.': {'NNP', 'JJ', 'RBS', 'VB', 'NNS', 'FW', 'NN', 'VBZ', 'VBP'}, 'u.s.a.': {'NN'}}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'s': ['NN']}\n",
      "tagged token without dot : \n",
      "{'ussr': ['NN'], 'dr': ['NN'], 'gen': ['NN'], 'messrs': ['NN'], 'pm': ['NN'], 'mr': ['NN'], 'usa': ['NN']}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'s\": {'POS'}}\n",
      "tagged token with dot : \n",
      "{'dr.': {'VBZ', 'NN', 'VBP', 'JJ'}, 'u.s.s.r.': {'JJ'}, 'prof.': {'NN'}, 'jr.': {'NN'}, 'maj.': {'NN'}, 's.s.r': {'NN'}, 'st.': {'JJ'}, 'a.m.': {'VBD', 'NN', 'RB'}, 'mrs.': {'NN'}, 'p.m.': {'NN', 'VBP', 'RB'}, 'u.n.r.r.a': {'RB', 'JJ'}, 'mr.': {'NNP', 'JJ', 'RBS', 'VB', 'NNS', 'FW', 'VBD', 'NN', 'RBR', 'RB', 'VBZ', 'VBP'}}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'s': ['NN']}\n",
      "tagged token without dot : \n",
      "{'dr': ['NN'], 'ussr': ['NN'], 'prof': ['NN'], 'jr': ['NN'], 'maj': ['NN'], 'ssr': ['NN'], 'st': ['NN'], 'am': ['VBP'], 'mrs': ['NN'], 'pm': ['NN'], 'unrra': ['NN'], 'mr': ['NN']}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. adress POS of token with symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_1_ = convert_pos(data=tagged_1, key=\".\", target_pos=\"NN\")\n",
    "tagged_2_ = convert_pos(data=tagged_2, key=\".\",  target_pos=\"NN\")\n",
    "tagged_3_ = convert_pos(data=tagged_3, key=\".\",  target_pos=\"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. Change POS to correct POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_correction_dict_1 = {'russia' : 'NN', 'germany' : 'NN', 'america' : 'NN', 'soviet' : 'NN'}\n",
    "pos_correction_dict_2 = {'bomb' : 'NN', 'america' : 'NN', 'soviet' : 'NN'}\n",
    "pos_correction_dict_3 = {'korea' : 'NN', 'america' : 'NN', 'soviet' : 'NN'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_tagged_1 = pos_correction(tagged_1_, pos_correction_dict_1)\n",
    "corrected_tagged_2 = pos_correction(tagged_2_, pos_correction_dict_2)\n",
    "corrected_tagged_3 = pos_correction(tagged_3_, pos_correction_dict_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-7. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_1, pos_table=pos_table)\n",
    "lemma_dict = {'saw' : 'see', 'men' : 'man'}\n",
    "lemmatized_1_all = lemmatizer.lemmatize(lemma_dict=lemma_dict)\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_2, pos_table=pos_table)\n",
    "lemmatized_2_all = lemmatizer.lemmatize()\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_3, pos_table=pos_table)\n",
    "lemmatized_3_all = lemmatizer.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_1, pos_table=pos_table, allowed_pos=['noun'])\n",
    "lemma_dict = {'saw' : 'see', 'men' : 'man'}\n",
    "lemmatized_1_noun = lemmatizer.lemmatize(lemma_dict=lemma_dict)\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_2, pos_table=pos_table, allowed_pos=['noun'])\n",
    "lemmatized_2_noun = lemmatizer.lemmatize()\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_3, pos_table=pos_table, allowed_pos=['noun'])\n",
    "lemmatized_3_noun = lemmatizer.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_1, pos_table=pos_table, allowed_pos=['verb'])\n",
    "lemma_dict = {'saw' : 'see', 'men' : 'man'}\n",
    "lemmatized_1_verb = lemmatizer.lemmatize(lemma_dict=lemma_dict)\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_2, pos_table=pos_table, allowed_pos=['verb'])\n",
    "lemmatized_2_verb = lemmatizer.lemmatize()\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_3, pos_table=pos_table, allowed_pos=['verb'])\n",
    "lemmatized_3_verb = lemmatizer.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_1, pos_table=pos_table, allowed_pos=['adjective'])\n",
    "lemma_dict = {'saw' : 'see', 'men' : 'man'}\n",
    "lemmatized_1_adjective = lemmatizer.lemmatize(lemma_dict=lemma_dict)\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_2, pos_table=pos_table, allowed_pos=['adjective'])\n",
    "lemmatized_2_adjective = lemmatizer.lemmatize()\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_3, pos_table=pos_table, allowed_pos=['adjective'])\n",
    "lemmatized_3_adjective = lemmatizer.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_1, pos_table=pos_table, allowed_pos=['adverb'])\n",
    "lemma_dict = {'saw' : 'see', 'men' : 'man'}\n",
    "lemmatized_1_adverb = lemmatizer.lemmatize(lemma_dict=lemma_dict)\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_2, pos_table=pos_table, allowed_pos=['adverb'])\n",
    "lemmatized_2_adverb = lemmatizer.lemmatize()\n",
    "\n",
    "lemmatizer = lemmatization_spacy(data=corrected_tagged_3, pos_table=pos_table, allowed_pos=['adverb'])\n",
    "lemmatized_3_adverb = lemmatizer.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save PreProcessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_ROOT = './processed-data/'\n",
    "\n",
    "SAVE_1 = SAVE_ROOT + 'period-1/ST/'\n",
    "SAVE_2 = SAVE_ROOT + 'period-2/ST/'\n",
    "SAVE_3 = SAVE_ROOT + 'period-3/ST/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. Preprocessed data to pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_all, file_name=\"lemmatized-all\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_all, file_name=\"lemmatized-all\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_all, file_name=\"lemmatized-all\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_noun, file_name=\"lemmatized-noun\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_noun, file_name=\"lemmatized-noun\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_noun, file_name=\"lemmatized-noun\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_verb, file_name=\"lemmatized-verb\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_verb, file_name=\"lemmatized-verb\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_verb, file_name=\"lemmatized-verb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_adjective, file_name=\"lemmatized-adjective\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_adjective, file_name=\"lemmatized-adjective\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_adjective, file_name=\"lemmatized-adjective\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_adverb, file_name=\"lemmatized-adverb\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_adverb, file_name=\"lemmatized-adverb\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_adverb, file_name=\"lemmatized-adverb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Preprocessed data to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(data=lemmatized_1_all, file_name=\"lemmatized-all\", root=SAVE_1)\n",
    "to_csv(data=lemmatized_2_all, file_name=\"lemmatized-all\", root=SAVE_2)\n",
    "to_csv(data=lemmatized_3_all, file_name=\"lemmatized-all\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(data=lemmatized_1_noun, file_name=\"lemmatized-noun\", root=SAVE_1)\n",
    "to_csv(data=lemmatized_2_noun, file_name=\"lemmatized-noun\", root=SAVE_2)\n",
    "to_csv(data=lemmatized_3_noun, file_name=\"lemmatized-noun\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_verb, file_name=\"lemmatized-verb\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_verb, file_name=\"lemmatized-verb\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_verb, file_name=\"lemmatized-verb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(data=lemmatized_1_adjective, file_name=\"lemmatized-adjective\", root=SAVE_1)\n",
    "to_csv(data=lemmatized_2_adjective, file_name=\"lemmatized-adjective\", root=SAVE_2)\n",
    "to_csv(data=lemmatized_3_adjective, file_name=\"lemmatized-adjective\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(data=lemmatized_1_adverb, file_name=\"lemmatized-adverb\", root=SAVE_1)\n",
    "to_csv(data=lemmatized_2_adverb, file_name=\"lemmatized-adverb\", root=SAVE_2)\n",
    "to_csv(data=lemmatized_3_adverb, file_name=\"lemmatized-adverb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Tagged data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=tagged_1_, file_name=\"tagged\", root=SAVE_1)\n",
    "to_pickle(data=tagged_2_, file_name=\"tagged\", root=SAVE_2)\n",
    "to_pickle(data=tagged_3_, file_name=\"tagged\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(data=tagged_1_, file_name=\"tagged\", root=SAVE_1)\n",
    "to_csv(data=tagged_2_, file_name=\"tagged\", root=SAVE_2)\n",
    "to_csv(data=tagged_3_, file_name=\"tagged\", root=SAVE_3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a3111211be4281f3a8c4a9b25563b8d253df502c7e31f5318895c1792a97cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
