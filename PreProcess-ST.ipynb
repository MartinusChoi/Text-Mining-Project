{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Data PreProcessing\n",
    "\n",
    "## [주요 고려 사항]\n",
    "1. `dot(.)`과 `apostrophe(')` 처리\n",
    "    - 'u.s.'와 'u.s.s.r.'과 같은 약자처리를 어떻게 할 것인가?\n",
    "    - 'america's'와 같은 소유격을 어떻게 처리할 것인가?\n",
    "        1. 처음 Cleaning 때, `dot(.)`과 `apostrophe(')`는 제거하지 않음\n",
    "            - `dot(.)`\n",
    "                - 'u.s', 'u.s.s.r'과 같은 약자를 유지시키기 위한 처리\n",
    "            - `apostrophe(')`\n",
    "                - 'america's'와 같은 소유격을 유지시켜서 Tokenizing때 's를 분리시키기 위함.\n",
    "        2. Tokenizing 이후, `dot(.)`과 `apostrophe(')`를 유지시켜야 하는 Token들 외에는 특수문자 제거\n",
    "            1. `apostrophe(')`와 `dot(.)`을 가진 Token들을 출력해보고 유지시킬 Token들의 목록을 결정\n",
    "            2. `apostrophe(')`를 유지시킬 Token들 외의 모든 Token들에서 `apostrophe(')` 및 특수문자 제거\n",
    "                - `dot(.)`은 다음 단계에서 예외처리를 하며 제거해야 하므로, 이 단계에서는 모든 `dot(.)`을 유지시킴\n",
    "            3. `dot(.)`을 유지시킬 Token들 외의 모든 Token들에서 `dot(.)` 및 특수문자 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined Modules\n",
    "from myModules.utils.data.DataLoader import DataLoader\n",
    "from myModules.utils.merge.mergeOverPeriod import merge\n",
    "from myModules.preprocess import cleaning, removeStopWords_ST, tagging, extract_some_pos_ST\n",
    "\n",
    "# General Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read File\n",
    "import glob\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = './Data/3구간/'\n",
    "\n",
    "PERIOD_1 = DATA_ROOT + '1시기/1시기_ST/'\n",
    "PERIOD_2 = DATA_ROOT + '2시기/2시기_ST/'\n",
    "PERIOD_3 = DATA_ROOT + '3시기/3시기_ST/'\n",
    "\n",
    "RESULT_ROOT = './Result/3구간/'\n",
    "\n",
    "RESULT_1 = RESULT_ROOT + '/1시기/ST/'\n",
    "RESULT_2 = RESULT_ROOT + '/2시기/ST/'\n",
    "RESULT_3 = RESULT_ROOT + '/3시기/ST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_1 = glob.glob(PERIOD_1+'*.txt')\n",
    "files_2 = glob.glob(PERIOD_2+'*.txt')\n",
    "files_3 = glob.glob(PERIOD_3+'*.txt')\n",
    "\n",
    "texts_1 = DataLoader(files_1, mode='ST')\n",
    "texts_2 = DataLoader(files_2, mode='ST')\n",
    "texts_3 = DataLoader(files_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Data Cleaning\n",
    "\n",
    "- `dot(.)`과 `apostrophe(')`는 제거하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_1 = cleaning(texts=texts_1, mode='ST')\n",
    "cleaned_2 = cleaning(texts=texts_2, mode='ST')\n",
    "cleaned_3 = cleaning(texts=texts_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dot_and_apostrophe:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def token_with_apostrophe(self):\n",
    "        apostrophe = []\n",
    "\n",
    "        for tokens in self.data:\n",
    "            for token in tokens:\n",
    "                if \"'\" in token : apostrophe.append(token)\n",
    "        \n",
    "        self.apostrophes = set(apostrophe)\n",
    "\n",
    "        print(f\"apostrophe를 가진 token : \\n{self.apostrophes}\")\n",
    "    \n",
    "    def token_with_dot(self):\n",
    "        dot = []\n",
    "\n",
    "        for tokens in self.data:\n",
    "            for token in tokens:\n",
    "                if \".\" in token : dot.append(token)\n",
    "        \n",
    "        self.dots = set(dot)\n",
    "\n",
    "        print(f\"dot을 가진 token : \\n{self.dots}\")\n",
    "        \n",
    "    def set_exception(self, apostrophe_exception, dot_exception):\n",
    "        self.apostrophe_exception = apostrophe_exception\n",
    "        self.dot_exception = dot_exception\n",
    "    \n",
    "    def print_exception(self):\n",
    "        print(f\"apostrophe exceptions : \\n{self.apostrophe_exception}\")\n",
    "        print(f\"dot exceptions : \\n{self.dot_exception}\")\n",
    "    \n",
    "    def remove_apostrophe(self, data):\n",
    "        result = []\n",
    "        processed = []\n",
    "\n",
    "        for tokens in data:\n",
    "            arr = []\n",
    "            for token in tokens:\n",
    "                if token not in self.apostrophe_exception:\n",
    "                    if not token.isalnum() : \n",
    "                        if \".\" not in token : processed.append(token)\n",
    "                    # dot은 삭제하지 않음. -> 예외처리하면서 삭제해야함\n",
    "                    arr.append(re.sub(\"[^a-z\\.]\", \"\", token))\n",
    "                else : arr.append(token)\n",
    "            result.append(arr)\n",
    "        \n",
    "        processed = set(processed)\n",
    "\n",
    "        print(f\"Processed Tokens : \\n{processed}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def remove_dot(self, data):\n",
    "        result = []\n",
    "        processed = []\n",
    "\n",
    "        for tokens in data:\n",
    "            arr = []\n",
    "            for token in tokens:\n",
    "                if token not in self.dot_exception:\n",
    "                    if not token.isalnum() : \n",
    "                        if \"'\" not in token : processed.append(token)\n",
    "                    # apostrophe는 삭제하지 않음. -> 예외처리하면서 삭제\n",
    "                    arr.append(re.sub(\"[^a-z']\", \"\", token))\n",
    "                else : arr.append(token)\n",
    "            result.append(arr)\n",
    "        \n",
    "        processed = set(processed)\n",
    "\n",
    "        print(f\"Processed Tokens : \\n{processed}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_invalid_tokens(self, data):\n",
    "        # 예외처리한 Token들 외에 특수문자를 가진 Token들이 있는지 확인\n",
    "        invalid_tokens = []\n",
    "\n",
    "        for tokens in data:\n",
    "            for token in tokens:\n",
    "                if not token.isalnum() : invalid_tokens.append(token)\n",
    "                elif len(token) == 1 : invalid_tokens.append(token)\n",
    "        \n",
    "        invalid_tokens = set(invalid_tokens)\n",
    "        exception = set(self.apostrophe_exception).union(set(self.dot_exception))\n",
    "        self.invalid_symbol = invalid_tokens.difference(exception)\n",
    "\n",
    "        if len(self.invalid_symbol) == 0:\n",
    "            print(\"There is no invalid symbol\")\n",
    "        else :\n",
    "            print(f\"Remaining invalid Symbol : {self.invalid_symbol}\")\n",
    "    \n",
    "    def remove_invalid_tokens(self, data):\n",
    "        # 남아있는 특수문자 + 길이가 1인 token들을 삭제\n",
    "        \n",
    "        result = []\n",
    "        removed = []\n",
    "\n",
    "        for tokens in data:\n",
    "            arr = []\n",
    "            for token in tokens:\n",
    "                if len(token) == 1 : removed.append(token)\n",
    "                elif token in self.invalid_symbol : removed.append(token)\n",
    "                else : arr.append(token)\n",
    "            result.append(arr)\n",
    "\n",
    "        removed = set(removed)\n",
    "        \n",
    "        print(f\"Removed Tokens : \\n{removed}\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_1 = [word_tokenize(text) for text in cleaned_1]\n",
    "tokenized_2 = [word_tokenize(text) for text in cleaned_2]\n",
    "tokenized_3 = [word_tokenize(text) for text in cleaned_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'structure\", \"'liberty\", \"n't\", \"'german\", \"'d\", \"'ve\", \"'ll\", \"'heat\", \"'s\", \"'blamed\", \"'democracy\", \"'mvd\", \"o'clock\", \"'m\", \"'\", \"'into\", \"'madam\", \"'system\", \"'are\"}\n",
      "dot을 가진 token : \n",
      "{'p.m.', 'col.', 'camps.if', 'u.s.s.r.', 'e.', 'w.', '...', 'gen.', 'u.s.', '..', 'u.', 'ph.d.', 'frightened.to', 'jr.', 'f.', 's.', '.', 'u.n.', 'i.', 'n.', 't.', 'a.', 'mrs.', 'a.m.', 'messrs.', 'v.', 'm.', 'oct.', 'dr.', 'mr.', 'co.', 'p.', 'st.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'ll\", \"'s\", \"'ve\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"messrs.\", \"gen.\", \"u.n.\", \"a.m.\", \"st.\", \"u.s.\", \"ph.d\", \"jr.\", \"p.m.\", \"mrs.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'ll\", \"'s\", \"'ve\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 'messrs.', 'gen.', 'u.n.', 'a.m.', 'st.', 'u.s.', 'ph.d', 'jr.', 'p.m.', 'mrs.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"o'clock\", \"'german\", \"'into\", \"'d\", \"'mvd\", \"'madam\", \"'heat\", \"'system\", \"'blamed\", \"'m\", \"'democracy\", \"'structure\", \"'are\", \"'liberty\", \"'\", \"n't\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1_ = symbol.remove_apostrophe(data=tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'col.', 'camps.if', 'e.', 'w.', '...', '..', 'u.', 'ph.d.', 'frightened.to', 'f.', 's.', '.', 'i.', 'n.', 't.', 'a.', 'v.', 'm.', 'oct.', 'co.', 'p.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1__ = symbol.remove_dot(data=tokenized_1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 token 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'a', 'b', 'h', 'n', 'g', 'o', 't', 'y', 'v', 'e', 'r', 'p', 'u', 's', 'm', 'w', 'd', 'f', 'x', 'k', 'i', 'j'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_1__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 Token들 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'a', 'b', 'h', 'n', 'g', 'o', 't', 'y', 'v', 'e', 'r', 'p', 'u', 's', 'm', 'w', 'd', 'f', 'x', 'k', 'i', 'j'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1___ = symbol.remove_invalid_tokens(data=tokenized_1__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 invalid한 token이 있는지 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_1___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peiod 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(tokenized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"'s\", \"'m\", \"'reprisals\", \"'for\", \"'\", \"n't\"}\n",
      "dot을 가진 token : \n",
      "{'p.m.', 'u.s.s.r.', 'e.', 'w.', 'gen.', '..', 'u.s.a.', 'r.', 'g.', 's.', '.', 'i.', 'n.', 't.', 'a.', 'c.', 'b.', 'messrs.', 'v.', 'm.', 'l.', 'h.', 'tyranny.the', 'mr.', 'dr.', 'p.', 'o.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'s\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"messrs.\", \"gen.\", \"u.s.a.\", \"p.m.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'s\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 'messrs.', 'gen.', 'u.s.a.', 'p.m.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"'m\", \"'reprisals\", \"'for\", \"'\", \"n't\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2_ = symbol.remove_apostrophe(data=tokenized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'e.', 'w.', '..', 'r.', 'g.', 's.', '.', 'i.', 'n.', 't.', 'a.', 'c.', 'b.', 'v.', 'm.', 'l.', 'h.', 'tyranny.the', 'p.', 'o.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2__ = symbol.remove_dot(data=tokenized_2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 Token들 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'a', 'b', 'c', 'h', 'n', 'g', 'o', 't', 'v', 'e', 'r', 'p', 's', 'm', 'd', 'w', 'l', 'f', 'i'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_2__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 token 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'a', 'b', 'c', 'h', 'n', 'g', 'o', 't', 'v', 'e', 'r', 'p', 's', 'm', 'd', 'w', 'l', 'f', 'i'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2___ = symbol.remove_invalid_tokens(data=tokenized_2__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 Invalid한 Token이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(data=tokenized_2___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = dot_and_apostrophe(tokenized_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe와 dot을 가진 token들 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe를 가진 token : \n",
      "{\"o'clock\", \"'d\", \"'ve\", \"'ll\", \"'vas\", \"'s\", \"'has\", \"'spontaneous\", \"'recession\", \"'\", \"n't\"}\n",
      "dot을 가진 token : \n",
      "{'p.m.', 'col.', 'u.n.r.r.a', '..................', 'u.s.s.r.', 'e.', 'w.', '...', 'gen.', 'u.', 'prof.', 'r.', 'jr.', 'g.', 'f.', 's.', '.', 'i.', 'n.', 't.', 'c.', 'a.', 'mrs.', 'a.m.', 'b.', 'v.', 'm.', 'maj.', 'l.', 'h.', 'dr.', 'mr.', 'p.', 's.s.r', 'st.', 'j.', 'o.', 'd.'}\n"
     ]
    }
   ],
   "source": [
    "symbol.token_with_apostrophe()\n",
    "symbol.token_with_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exception 목록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_exception = [\"'ll\", \"'s\", \"'ve\"]\n",
    "dot_exception = [\"u.s.s.r.\", \"dr.\", \"s.s.r\", \"a.m.\", \"st.\", \"prof.\", \"u.n.r.r.a\", \"jr.\", \"maj.\", \"p.m.\", \"mrs.\", \"mr.\"]\n",
    "\n",
    "symbol.set_exception(apostrophe_exception=apostrophe_exception, dot_exception=dot_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apostrophe exceptions : \n",
      "[\"'ll\", \"'s\", \"'ve\"]\n",
      "dot exceptions : \n",
      "['u.s.s.r.', 'dr.', 's.s.r', 'a.m.', 'st.', 'prof.', 'u.n.r.r.a', 'jr.', 'maj.', 'p.m.', 'mrs.', 'mr.']\n"
     ]
    }
   ],
   "source": [
    "symbol.print_exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apostrophe 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{\"o'clock\", \"'d\", \"'vas\", '``', \"'has\", \"'spontaneous\", \"'recession\", \"'\", \"n't\"}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3_ = symbol.remove_apostrophe(tokenized_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens : \n",
      "{'', 'col.', '..................', 'e.', 'w.', '...', 'gen.', 'u.', 'r.', 'g.', 'f.', 's.', '.', 'i.', 'n.', 't.', 'c.', 'a.', 'b.', 'v.', 'm.', 'l.', 'h.', 'p.', 'j.', 'o.', 'd.'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3__ = symbol.remove_dot(tokenized_3_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제거해야할 token 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid Symbol : {'', 'a', 'b', 'c', 'h', 'n', 'g', 'o', 't', 'v', 'e', 'r', 'p', 'u', 's', 'm', 'w', 'd', 'l', 'f', 'x', 'i', 'j'}\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(tokenized_3__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 길이가 1이거나 필요없는 특수문자인 token 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Tokens : \n",
      "{'', 'a', 'b', 'c', 'h', 'n', 'g', 'o', 't', 'v', 'e', 'r', 'p', 'u', 's', 'm', 'd', 'w', 'l', 'f', 'x', 'i', 'j'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3___ = symbol.remove_invalid_tokens(tokenized_3__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 남아있는 INvalid한 token이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no invalid symbol\n"
     ]
    }
   ],
   "source": [
    "symbol.check_invalid_tokens(tokenized_3___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['would', 'could', 'might', 'need', 'can', 'must', \\\n",
    "    'one', 'two', 'upon', 'may', 'perhaps', 'living', 'seem', 'also', 'ii', 'ofthe',\n",
    "    'also', 'much', 'therefore']\n",
    "\n",
    "wo_stopword_1 = removeStopWords_ST(tokenized_1___, stopwords, new_stopwords)\n",
    "wo_stopword_2 = removeStopWords_ST(tokenized_2___, stopwords, new_stopwords)\n",
    "wo_stopword_3 = removeStopWords_ST(tokenized_3___, stopwords, new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_table = pd.read_pickle(\"processed-data/pos-table.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4f047f3274463bbd6e28bdca724d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35133813603349b7b68868823264ab68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91322f2c8a78439686d280077bbde8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged_1 = tagging(wo_stopword_1, mode='ST')\n",
    "tagged_2 = tagging(wo_stopword_2, mode='ST')\n",
    "tagged_3 = tagging(wo_stopword_3, mode='ST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_pos:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.dots = {}\n",
    "        self.apostrophes = {}\n",
    "        self.dots_wo = {}\n",
    "        self.apostrophes_wo = {}\n",
    "\n",
    "        arr_dot = []\n",
    "        arr_apostrophe = []\n",
    "\n",
    "        for tags in self.data:\n",
    "            for tag in tags:\n",
    "                if \".\" in tag[0] : arr_dot.append(tag[0])\n",
    "                elif \"'\" in tag[0] : arr_apostrophe.append(tag[0])\n",
    "        \n",
    "        for dot in set(arr_dot):\n",
    "            self.dots[dot] = set([tag[1] for tag in merge(self.data) if tag[0] == dot])\n",
    "        \n",
    "        for apos in set(arr_apostrophe):\n",
    "            self.apostrophes[apos] = set([tag[1] for tag in merge(self.data) if tag[0] == apos])\n",
    "        \n",
    "        for dot in set(arr_dot):\n",
    "            removed = nltk.pos_tag([re.sub(\"[^a-z]\", \"\", dot)])\n",
    "            self.dots_wo[removed[0][0]] = [removed[0][1]]\n",
    "        \n",
    "        for apos in set(arr_apostrophe):\n",
    "            removed = nltk.pos_tag([re.sub(\"[^a-z]\", \"\", apos)])\n",
    "            self.apostrophes_wo[removed[0][0]] = [removed[0][1]]\n",
    "        \n",
    "    \n",
    "    def pos_with_symbol(self):\n",
    "        print(f\"tagged token with apostrophe : \\n{self.apostrophes}\")\n",
    "        print(f\"tagged token with dot : \\n{self.dots}\") \n",
    "\n",
    "    def pos_without_symbol(self):\n",
    "        print(f\"tagged token without apostrophe : \\n{self.apostrophes_wo}\")\n",
    "        print(f\"tagged token without dot : \\n{self.dots_wo}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'ve\": {'VBP'}, \"'s\": {'POS'}, \"'ll\": {'MD'}}\n",
      "tagged token with dot : \n",
      "{'p.m.': {'RB'}, 'u.s.': {'JJ'}, 'u.n.': {'NN'}, 'dr.': {'JJ', 'VBP'}, 'mr.': {'NN', 'NNP', 'RBS', 'RB', 'VBP', 'JJ'}, 'u.s.s.r.': {'JJ', 'VBP'}, 'mrs.': {'NNS'}, 'st.': {'NN', 'JJ'}, 'a.m.': {'JJ'}, 'messrs.': {'NN'}, 'jr.': {'NN', 'VBP'}, 'gen.': {'NN', 'JJ', 'VBP'}}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'ve': ['NN'], 's': ['NN'], 'll': ['NN']}\n",
      "tagged token without dot : \n",
      "{'pm': ['NN'], 'us': ['PRP'], 'un': ['NN'], 'dr': ['NN'], 'mr': ['NN'], 'ussr': ['NN'], 'mrs': ['NN'], 'st': ['NN'], 'am': ['VBP'], 'messrs': ['NN'], 'jr': ['NN'], 'gen': ['NN']}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'s\": {'POS'}}\n",
      "tagged token with dot : \n",
      "{'p.m.': {'JJ'}, 'dr.': {'NN'}, 'u.s.a.': {'NN'}, 'mr.': {'NN', 'NNP', 'RBS', 'VBZ', 'FW', 'VBP', 'NNS', 'JJ', 'VB'}, 'u.s.s.r.': {'JJ'}, 'messrs.': {'NNS'}, 'gen.': {'JJ'}}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'s': ['NN']}\n",
      "tagged token without dot : \n",
      "{'pm': ['NN'], 'dr': ['NN'], 'usa': ['NN'], 'mr': ['NN'], 'ussr': ['NN'], 'messrs': ['NN'], 'gen': ['NN']}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = check_pos(tagged_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token with apostrophe : \n",
      "{\"'ve\": {'VBP'}, \"'s\": {'POS'}, \"'ll\": {'MD'}}\n",
      "tagged token with dot : \n",
      "{'p.m.': {'NN', 'RB', 'VBP'}, 'maj.': {'NN'}, 'u.n.r.r.a': {'RB', 'JJ'}, 'dr.': {'NN', 'JJ', 'VBZ', 'VBP'}, 'mr.': {'NN', 'VBD', 'NNP', 'RBS', 'RBR', 'VBZ', 'FW', 'RB', 'VBP', 'NNS', 'JJ', 'VB'}, 'u.s.s.r.': {'JJ'}, 's.s.r': {'NN'}, 'mrs.': {'NN'}, 'st.': {'JJ'}, 'prof.': {'NN'}, 'a.m.': {'VBD', 'RB', 'NN'}, 'jr.': {'NN'}}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_with_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged token without apostrophe : \n",
      "{'ve': ['NN'], 's': ['NN'], 'll': ['NN']}\n",
      "tagged token without dot : \n",
      "{'pm': ['NN'], 'maj': ['NN'], 'unrra': ['NN'], 'dr': ['NN'], 'mr': ['NN'], 'ussr': ['NN'], 'ssr': ['NN'], 'mrs': ['NN'], 'st': ['NN'], 'prof': ['NN'], 'am': ['VBP'], 'jr': ['NN']}\n"
     ]
    }
   ],
   "source": [
    "pos.pos_without_symbol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. adress POS of token with symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pos(data, target_pos=\"NN\"):\n",
    "    result = []\n",
    "\n",
    "    for tags in data:\n",
    "        arr = []\n",
    "        for tag in tags:\n",
    "            if \".\" in tag[0] : arr.append((tag[0], target_pos))\n",
    "            else : arr.append(tag)\n",
    "        result.append(arr)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_1_ = convert_pos(data=tagged_1, target_pos=\"NN\")\n",
    "tagged_2_ = convert_pos(data=tagged_2, target_pos=\"NN\")\n",
    "tagged_3_ = convert_pos(data=tagged_3, target_pos=\"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmatization:\n",
    "    def __init__(self, data, lemmatizer, pos_table, allowed_pos=['noun', 'verb', 'adjective', 'adverb']):\n",
    "        self.data = data\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.allowed_pos = []\n",
    "        for pos in allowed_pos:\n",
    "            if pos == 'noun' : self.allowed_pos.extend(pos_table.Eng_tag[0])\n",
    "            elif pos == 'verb' : self.allowed_pos.extend(pos_table.Eng_tag[2])\n",
    "            elif pos == 'adjective' : self.allowed_pos.extend(pos_table.Eng_tag[3])\n",
    "            elif pos == 'adverb' : self.allowed_pos.extend(pos_table.Eng_tag[4])\n",
    "    \n",
    "    def append(self, token, arr):\n",
    "        if \".\" in token : arr.append(token)\n",
    "        elif \"'\" in token : arr.append(token)\n",
    "        elif token == \"us\" : arr.append(token)\n",
    "        else : \n",
    "            arr.append(self.lemmatizer.lemmatize(token))\n",
    "        \n",
    "        return arr\n",
    "\n",
    "    def lemmatize(self):\n",
    "        result = []\n",
    "\n",
    "        for tags in self.data:\n",
    "            arr = []\n",
    "            for token, pos in tags:\n",
    "                if pos in self.allowed_pos : self.append(token, arr)\n",
    "            result.append(arr)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['noun', 'verb', 'adjective', 'adverb'])\n",
    "lemmatized_1_all = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['noun', 'verb', 'adjective', 'adverb'])\n",
    "lemmatized_2_all = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['noun', 'verb', 'adjective', 'adverb'])\n",
    "lemmatized_3_all = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['noun'])\n",
    "lemmatized_1_noun = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['noun'])\n",
    "lemmatized_2_noun = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['noun'])\n",
    "lemmatized_3_noun = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['verb'])\n",
    "lemmatized_1_verb = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['verb'])\n",
    "lemmatized_2_verb = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['verb'])\n",
    "lemmatized_3_verb = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['adjective'])\n",
    "lemmatized_1_adjective = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['adjective'])\n",
    "lemmatized_2_adjective = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['adjective'])\n",
    "lemmatized_3_adjective = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize = lemmatization(tagged_1, lemmatizer, pos_table, allowed_pos=['adverb'])\n",
    "lemmatized_1_adverb = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_2, lemmatizer, pos_table, allowed_pos=['adverb'])\n",
    "lemmatized_2_adverb = lemmatize.lemmatize()\n",
    "\n",
    "lemmatize = lemmatization(tagged_3, lemmatizer, pos_table, allowed_pos=['adverb'])\n",
    "lemmatized_3_adverb = lemmatize.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save PreProcessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_ROOT = './processed-data/'\n",
    "\n",
    "SAVE_1 = SAVE_ROOT + 'period-1/'\n",
    "SAVE_2 = SAVE_ROOT + 'period-2/'\n",
    "SAVE_3 = SAVE_ROOT + 'period-3/'\n",
    "\n",
    "def to_pickle(data, file_name, root='./'):\n",
    "    with open(root+file_name+'.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatized data to pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_all, file_name=\"lemmatized-all\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_all, file_name=\"lemmatized-all\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_all, file_name=\"lemmatized-all\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_noun, file_name=\"lemmatized-noun\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_noun, file_name=\"lemmatized-noun\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_noun, file_name=\"lemmatized-noun\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_verb, file_name=\"lemmatized-verb\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_verb, file_name=\"lemmatized-verb\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_verb, file_name=\"lemmatized-verb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_adjective, file_name=\"lemmatized-adjective\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_adjective, file_name=\"lemmatized-adjective\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_adjective, file_name=\"lemmatized-adjective\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=lemmatized_1_adverb, file_name=\"lemmatized-adverb\", root=SAVE_1)\n",
    "to_pickle(data=lemmatized_2_adverb, file_name=\"lemmatized-adverb\", root=SAVE_2)\n",
    "to_pickle(data=lemmatized_3_adverb, file_name=\"lemmatized-adverb\", root=SAVE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagged data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle(data=tagged_1_, file_name=\"tagged\", root=SAVE_1)\n",
    "to_pickle(data=tagged_2_, file_name=\"tagged\", root=SAVE_2)\n",
    "to_pickle(data=tagged_3_, file_name=\"tagged\", root=SAVE_3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a3111211be4281f3a8c4a9b25563b8d253df502c7e31f5318895c1792a97cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
