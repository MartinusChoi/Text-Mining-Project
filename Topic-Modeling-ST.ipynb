{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeul Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined Modules\n",
    "from myModules.utils.data.DataLoader import DataLoader\n",
    "from myModules.utils.merge.mergeOverPeriod import merge\n",
    "from myModules.preprocess import cleaning, tagging, removeStopWords_ST, tokenizing_ST\n",
    "from myModules.TopicModeling.LDA.ldaModeling import buildDTM, topicWords, visualizeLDA\n",
    "\n",
    "# General Modules\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read File\n",
    "import glob\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from apyori import apriori\n",
    "from nltk import FreqDist\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = './Data/3구간/'\n",
    "\n",
    "PERIOD_1 = DATA_ROOT + '1시기/1시기_ST/'\n",
    "PERIOD_2 = DATA_ROOT + '2시기/2시기_ST/'\n",
    "PERIOD_3 = DATA_ROOT + '3시기/3시기_ST/'\n",
    "\n",
    "RESULT_ROOT = './Result/3구간/'\n",
    "\n",
    "RESULT_1 = RESULT_ROOT + '/1시기/ST/'\n",
    "RESULT_2 = RESULT_ROOT + '/2시기/ST/'\n",
    "RESULT_3 = RESULT_ROOT + '/3시기/ST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_1 = glob.glob(PERIOD_1+'*.txt')\n",
    "files_2 = glob.glob(PERIOD_2+'*.txt')\n",
    "files_3 = glob.glob(PERIOD_3+'*.txt')\n",
    "\n",
    "texts_1 = DataLoader(files_1, mode='ST')\n",
    "texts_2 = DataLoader(files_2, mode='ST')\n",
    "texts_3 = DataLoader(files_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_1 = cleaning(texts_1, mode='ST')\n",
    "cleaned_2 = cleaning(texts_2, mode='ST')\n",
    "cleaned_3 = cleaning(texts_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tokenized_1 = tokenizing_ST(cleaned_1, tokenizer)\n",
    "tokenized_2 = tokenizing_ST(cleaned_2, tokenizer)\n",
    "tokenized_3 = tokenizing_ST(cleaned_3, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['would', 'could', 'might', 'need', 'can', 'must', 'e']\n",
    "\n",
    "wo_stopword_1 = removeStopWords_ST(tokenized_1, stopwords, new_stopwords)\n",
    "wo_stopword_2 = removeStopWords_ST(tokenized_2, stopwords, new_stopwords)\n",
    "wo_stopword_3 = removeStopWords_ST(tokenized_3, stopwords, new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagList = [['noun', ['NN','NNS','NNP','NNPS'], ['NNG','NNB','NNP','NNM']], \\\n",
    "    ['pronoun', ['PRP','WP','PRP'], ['NP']],\n",
    "    ['verb', ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'], ['VV', 'VXV', 'VCP']],\n",
    "    ['adjective', ['JJ', 'JJR', 'JJS'], ['VA', 'VXA', 'VCN']],\n",
    "    ['adverb', ['RB', 'RBR', 'RBS', 'WRB', 'EX', 'RP'], ['MAG']],\n",
    "    ['prep&conj', ['TO', 'IN', 'CC'], ['MAC']],\n",
    "    ['determiner', ['DT', 'PDT', 'WDT'], ['MDT', 'MDN']],\n",
    "    ['interjection',['UH'], ['IC']],\n",
    "    ['number', ['CD'], ['NR', 'ON']],\n",
    "    ['foreignW', ['FW'],['OL']],\n",
    "    ['modal',['MD'],[]],\n",
    "    ['josa', [], ['JC', 'JK', 'JKC', 'JKG', 'JKI', 'JKM', 'JKO', 'JKQ', 'JKS', 'JX']],\n",
    "    ['possesiveS', ['POS'], []],\n",
    "    ['others',['LS'], ['EPH', 'EPT', 'EPP', 'EFN', 'EFQ', 'EFO', 'EFA', 'EFI', 'EFR', 'ECE', 'ECD', 'ECS', 'ETN', 'ETD', 'XPN', 'XPV', 'XSN', 'XSV', 'XSA', 'XR', 'UN', 'OH']]]\n",
    "\n",
    "tagList = pd.DataFrame(tagList)\n",
    "tagList.columns = ['POS', 'Eng_tag', 'Kor_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_1 = tagging(tokenized_1, mode='ST')\n",
    "tagged_2 = tagging(tokenized_2, mode='ST')\n",
    "tagged_3 = tagging(tokenized_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. parameter tuning\n",
    "\n",
    "- passes & iteration : 가능한 많은 것이 좋다.\n",
    "- topic : 가설로 설정한 topic의 갯수\n",
    "    1. Topic Coherence\n",
    "        - 주제의 일관성 측정\n",
    "        - 모델링이 잘 될수록 한 주제 안에는 의미론적으로 유사한 단어가 많이 모여있게 됨.\n",
    "        - 높을수록 의미론적 일관성이 높다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corp, Dict = buildDTM(wo_stopword_1)\n",
    "\n",
    "coherences = []\n",
    "passes = []\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for passes in range(0, 10, 5):\n",
    "    model = models.ldamodel.LdaModel(Corp, num_topics = NUM_TOPICS, id2word=Dict, iterations=400, passes=passes)\n",
    "\n",
    "data = visualizeLDA(model, Corp, Dict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a3111211be4281f3a8c4a9b25563b8d253df502c7e31f5318895c1792a97cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
