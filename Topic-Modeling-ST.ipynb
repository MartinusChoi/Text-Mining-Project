{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Data Topic Modeling Using `LDA`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeul Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined Modules\n",
    "from myModules.utils.data.DataLoader import DataLoader\n",
    "from myModules.utils.merge.mergeOverPeriod import merge\n",
    "from myModules.preprocess import cleaning, tagging, removeStopWords_ST, tokenizing_ST, extract_some_pos_ST, keras_tokenizer\n",
    "from myModules.TopicModeling.LDA.ldaModeling import buildDTM, topicWords, visualizeLDA\n",
    "\n",
    "# General Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read File\n",
    "import glob\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = './Data/3구간/'\n",
    "\n",
    "PERIOD_1 = DATA_ROOT + '1시기/1시기_ST/'\n",
    "PERIOD_2 = DATA_ROOT + '2시기/2시기_ST/'\n",
    "PERIOD_3 = DATA_ROOT + '3시기/3시기_ST/'\n",
    "\n",
    "RESULT_ROOT = './Result/3구간/'\n",
    "\n",
    "RESULT_1 = RESULT_ROOT + '/1시기/ST/'\n",
    "RESULT_2 = RESULT_ROOT + '/2시기/ST/'\n",
    "RESULT_3 = RESULT_ROOT + '/3시기/ST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_1 = glob.glob(PERIOD_1+'*.txt')\n",
    "files_2 = glob.glob(PERIOD_2+'*.txt')\n",
    "files_3 = glob.glob(PERIOD_3+'*.txt')\n",
    "\n",
    "texts_1 = DataLoader(files_1, mode='ST')\n",
    "texts_2 = DataLoader(files_2, mode='ST')\n",
    "texts_3 = DataLoader(files_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Data Cleaning\n",
    "\n",
    "- nltk.tokenize의 word_tokenize 는 's를 자동으로 분류해줌\n",
    "- 탐색 결과 어퍼스트로피로 끝나는 단어와 어퍼스트로피로 시작하는 단어도 존재\n",
    "- 따라서 이렇게 어퍼스트로피로 시작하거나 끝나는 단어는 특수문자를 제거\n",
    "- 's 는 삭제하지 않음\n",
    "- \"'\"와 \".\"는 cleaning시 제거하지 않고, 나머지 특수문자는 공백으로 제거\n",
    "    - \".\"는 u.s 나 u.s.s.r 과 같은 약자를 구분하기 위해 제거하지 않음\n",
    "    - nltk.tokenize의 word_tokenize 가 u.s. 를 u.s와 .로 구분해주기에 us와 u.s.를 구분할 수 있을 것이라고 기대할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_1 = cleaning(texts_1, mode='ST')\n",
    "cleaned_2 = cleaning(texts_2, mode='ST')\n",
    "cleaned_3 = cleaning(texts_3, mode='ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. tokenizing\n",
    "\n",
    "- nltk.tokenize의 word_tokenized를 이용하여 tokenizing 수행\n",
    "- 어퍼스트로피와 마침표를 제거하지 않고 넣어줬으므로, 소유격등 특수한 어휘의 특성을 유지하기위해 어퍼스트로피를 유지할 token을 설정\n",
    "- 어퍼스트로피를 유지할 Token외에는 모든 특수문자를 제거\n",
    "    - 이때 u.s를 위해 .는 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_apostrophe(data):\n",
    "    startwith = []\n",
    "    endwith = []\n",
    "\n",
    "    for tokenized in data:\n",
    "        for token in tokenized:\n",
    "            if token.endswith(\"'\"): endwith.append(token)\n",
    "            if token.startswith(\"'\"): startwith.append(token)\n",
    "\n",
    "    set_start = set(startwith)\n",
    "    set_end = set(endwith)\n",
    "\n",
    "    print(f\"Tokenize 이후 ' 로 시작하는 token : \\n{set_start}\")\n",
    "    print(f\"Tokneize 이후 ' 로 끝나는 token : \\n{set_end}\")\n",
    "\n",
    "def remove_apostrophe(data, exception):\n",
    "    result = []\n",
    "\n",
    "    for tokenized in data:\n",
    "        tokens = []\n",
    "        for token in tokenized:\n",
    "            if token not in exception:\n",
    "                tokens.append(re.sub(\"[^a-z]\", \"\", token))\n",
    "            else : tokens.append(token)\n",
    "        result.append(tokens)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def remove_special(data, exception):\n",
    "    not_alnum = []\n",
    "    result = []\n",
    "\n",
    "    for tokens in data:\n",
    "        for token in tokens:\n",
    "            if not token.isalnum() : not_alnum.append(token)\n",
    "    \n",
    "    remove_list = set(not_alnum)\n",
    "    exception = set(exception)\n",
    "    remove_list = remove_list.difference(exception)\n",
    "\n",
    "    print(f\"Removed : {remove_list}\")\n",
    "\n",
    "    for article in data:\n",
    "        tokens = []\n",
    "        for token in article:\n",
    "            if token not in remove_list:\n",
    "                tokens.append(token)\n",
    "        result.append(tokens)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_1 = [word_tokenize(text) for text in cleaned_1]\n",
    "tokenized_2 = [word_tokenize(text) for text in cleaned_2]\n",
    "tokenized_3 = [word_tokenize(text) for text in cleaned_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 이후 ' 로 시작하는 token : \n",
      "{\"'s\", \"'d\", \"'german\", \"'m\", \"'blamed\", \"'ve\", \"'into\", \"'structure\", \"'madam\", \"'are\", \"'heat\", \"'democracy\", \"'system\", \"'mvd\", \"'ll\", \"'liberty\", \"'\"}\n",
      "Tokneize 이후 ' 로 끝나는 token : \n",
      "{\"'\"}\n"
     ]
    }
   ],
   "source": [
    "test_apostrophe(tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'ll`, `'d`, `'s`, `'ve` 가 아니면 `'`를 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception = [\"'ll\", \"'d\", \"'s\", \"'ve\"]\n",
    "\n",
    "tokenized_1_ = remove_apostrophe(tokenized_1, exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 이후 ' 로 시작하는 token : \n",
      "{\"'s\", \"'ll\", \"'ve\", \"'d\"}\n",
      "Tokneize 이후 ' 로 끝나는 token : \n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "test_apostrophe(tokenized_1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed : {''}\n"
     ]
    }
   ],
   "source": [
    "tokenized_1_ = remove_special(tokenized_1_, exception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 이후 ' 로 시작하는 token : \n",
      "{\"'s\", \"'m\", \"'reprisals\", \"'for\", \"'\"}\n",
      "Tokneize 이후 ' 로 끝나는 token : \n",
      "{\"'\"}\n"
     ]
    }
   ],
   "source": [
    "test_apostrophe(tokenized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'s`가 아니면 `'`를 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception = [\"'s\"]\n",
    "\n",
    "tokenized_2_ = remove_apostrophe(tokenized_2, exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 이후 ' 로 시작하는 token : \n",
      "{\"'s\"}\n",
      "Tokneize 이후 ' 로 끝나는 token : \n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "test_apostrophe(tokenized_2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed : {''}\n"
     ]
    }
   ],
   "source": [
    "tokenized_2_ = remove_special(tokenized_2_, exception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peiod 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 이후 ' 로 시작하는 token : \n",
      "{\"'s\", \"'d\", \"'german\", \"'m\", \"'blamed\", \"'ve\", \"'into\", \"'structure\", \"'madam\", \"'are\", \"'heat\", \"'democracy\", \"'system\", \"'mvd\", \"'ll\", \"'liberty\", \"'\"}\n",
      "Tokneize 이후 ' 로 끝나는 token : \n",
      "{\"'\"}\n"
     ]
    }
   ],
   "source": [
    "test_apostrophe(tokenized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'d`, `'s`, `'ve`, `'ll`를 제외하고는 모두 `'` 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception = [\"'d\", \"'s\", \"'ve\", \"'ll\"]\n",
    "\n",
    "tokenized_3_ = remove_apostrophe(tokenized_3, exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 이후 ' 로 시작하는 token : \n",
      "{\"'s\", \"'ll\", \"'ve\", \"'d\"}\n",
      "Tokneize 이후 ' 로 끝나는 token : \n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "test_apostrophe(tokenized_3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed : {''}\n"
     ]
    }
   ],
   "source": [
    "tokenized_3_ = remove_special(tokenized_3_, exception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['would', 'could', 'might', 'need', 'can', 'must', \\\n",
    "    'e', 'one', 'two', 'upon', 'may', 'perhaps', 'living', 'seem', 'also', 'ii', 'ofthe',\n",
    "    'also', 'much', 'therefore', 'u', 's']\n",
    "\n",
    "wo_stopword_1 = removeStopWords_ST(tokenized_1_, stopwords, new_stopwords)\n",
    "wo_stopword_2 = removeStopWords_ST(tokenized_2_, stopwords, new_stopwords)\n",
    "wo_stopword_3 = removeStopWords_ST(tokenized_3_, stopwords, new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(data, lemmatizer):\n",
    "    result = []\n",
    "\n",
    "    for article in data:\n",
    "        result.append([lemmatizer.lemmatize(token) for token in article])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_1 = lemmatizing(wo_stopword_1, lemmatizer)\n",
    "lemmatized_2 = lemmatizing(wo_stopword_2, lemmatizer)\n",
    "lemmatized_3 = lemmatizing(wo_stopword_3, lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagList = [['noun', ['NN','NNS','NNP','NNPS'], ['NNG','NNB','NNP','NNM']], \\\n",
    "    ['pronoun', ['PRP','WP','PRP'], ['NP']],\n",
    "    ['verb', ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'], ['VV', 'VXV', 'VCP']],\n",
    "    ['adjective', ['JJ', 'JJR', 'JJS'], ['VA', 'VXA', 'VCN']],\n",
    "    ['adverb', ['RB', 'RBR', 'RBS', 'WRB', 'EX', 'RP'], ['MAG']],\n",
    "    ['prep&conj', ['TO', 'IN', 'CC'], ['MAC']],\n",
    "    ['determiner', ['DT', 'PDT', 'WDT'], ['MDT', 'MDN']],\n",
    "    ['interjection',['UH'], ['IC']],\n",
    "    ['number', ['CD'], ['NR', 'ON']],\n",
    "    ['foreignW', ['FW'],['OL']],\n",
    "    ['modal',['MD'],[]],\n",
    "    ['josa', [], ['JC', 'JK', 'JKC', 'JKG', 'JKI', 'JKM', 'JKO', 'JKQ', 'JKS', 'JX']],\n",
    "    ['possesiveS', ['POS'], []],\n",
    "    ['others',['LS'], ['EPH', 'EPT', 'EPP', 'EFN', 'EFQ', 'EFO', 'EFA', 'EFI', 'EFR', 'ECE', 'ECD', 'ECS', 'ETN', 'ETD', 'XPN', 'XPV', 'XSN', 'XSV', 'XSA', 'XR', 'UN', 'OH']]]\n",
    "\n",
    "tagList = pd.DataFrame(tagList)\n",
    "tagList.columns = ['POS', 'Eng_tag', 'Kor_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39a3835336e4db7aaa64923a3581b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7639a113a7bb45b6a4437614e9914967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ccfd21e10d4adfaec56a76c8fc0dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged_1 = tagging(lemmatized_1, mode='ST')\n",
    "tagged_2 = tagging(lemmatized_2, mode='ST')\n",
    "tagged_3 = tagging(lemmatized_3, mode='ST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_1 = extract_some_pos_ST(articles=tagged_1, tagList=tagList, pos_list=['noun', 'verb', 'adjective'])\n",
    "extracted_2 = extract_some_pos_ST(articles=tagged_2, tagList=tagList, pos_list=['noun', 'verb', 'adjective'])\n",
    "extracted_3 = extract_some_pos_ST(articles=tagged_3, tagList=tagList, pos_list=['noun', 'verb', 'adjective'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. parameter tuning & LDA modeling\n",
    "\n",
    "- topic num : 가설로 설정한 topic의 갯수\n",
    "    1. Topic Coherence\n",
    "        - 주제의 일관성 측정\n",
    "        - 모델링이 잘 될수록 한 주제 안에는 의미론적으로 유사한 단어가 많이 모여있게 됨.\n",
    "        - 높을수록 의미론적 일관성이 높다.\n",
    "        - Coherence가 높아지면 Monotonic 해지는 문제점이 생긴다.\n",
    "        - coherence가 너무 높아지면 정보의 양이 줄어들고, coherence가 너무 낮으면 정보들의 연관성이 없어져 분석의 의미가 없다.\n",
    "    2. Perplexity   \n",
    "        - Coherence가 이 data에서 topic number가 늘어날수록 거의 같이 늘어나는 경향을 보임\n",
    "        - 따라서 다른 평가기준도 함께 고려해야겠다는 생각에 추가\n",
    "        - 작아질수록 토픽모델이 문서를 잘 반영한다.\n",
    "\n",
    "- lda modeling 결과를 시각화해 보았을 때, 10이상으로 넘어가면 할당되지 않는 빈 id들이 발견되었습니다.\n",
    "    - 따라서 시험할 k값의 범위를 1~10까지 자연수로 설정하였습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPIC_WORDS = 30\n",
    "PASSES = 30\n",
    "ITERATIONS = 400\n",
    "EVAL_EVERY = None\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestLDAPram:\n",
    "    def __init__(self, passes=30, iterations=400, eval_every=None, random_state=42):\n",
    "        self.passes = passes\n",
    "        self.iterations = iterations\n",
    "        self.eval_every = eval_every\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def calc_coherence(self, corpus, dictionary, alpha='auto', eta='auto'):\n",
    "        self.coherences = []\n",
    "        self.num_topics_c = []\n",
    "\n",
    "        for ntopics in tqdm(range(1, 11), desc='Topic Coherence'):\n",
    "            model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=ntopics, \\\n",
    "                iterations=self.iterations, passes=self.passes, \\\n",
    "                alpha=alpha, eta=eta, eval_every=self.eval_every, random_state=self.random_state)\n",
    "            \n",
    "            cm = CoherenceModel(model=model, corpus=corpus, coherence='u_mass')\n",
    "            coherence = cm.get_coherence()\n",
    "\n",
    "            self.coherences.append(coherence)\n",
    "            self.num_topics_c.append(ntopics)\n",
    "    \n",
    "    def calc_perplexity(self, corpus, dictionary, alpha='auto', eta='auto'):\n",
    "        self.perplexities = []\n",
    "        self.num_topics_p = []\n",
    "\n",
    "        for ntopics in tqdm(range(1, 11), desc='Perpelxity'):\n",
    "            model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=ntopics, \\\n",
    "                iterations=self.iterations, passes=self.passes, \\\n",
    "                alpha=alpha, eta=eta, eval_every=self.eval_every, random_state=self.random_state)\n",
    "            \n",
    "            perplexity = model.log_perplexity(corpus)\n",
    "            self.perplexities.append(perplexity)\n",
    "            self.num_topics_p.append(ntopics)\n",
    "    \n",
    "    def calc_alpha(self, corpus, dictionary, num_topics, eta='auto'):\n",
    "        self.alphas = []\n",
    "        self.coherences_a = []\n",
    "\n",
    "        for i in tqdm(range(0, 11), desc='alpha'):\n",
    "            alpha = i * 0.001\n",
    "            model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, \\\n",
    "                iterations=self.iterations, passes=self.passes, \\\n",
    "                alpha=alpha, eta=eta, eval_every=self.eval_every, random_state=self.random_state)\n",
    "\n",
    "            cm = CoherenceModel(model=model, corpus=corpus, coherence='u_mass')\n",
    "            coherence = cm.get_coherence()\n",
    "\n",
    "            self.coherences_a.append(coherence)\n",
    "            self.alphas.append(alpha)\n",
    "    \n",
    "    def calc_eta(self, corpus, dictionary, num_topics, alpha='auto'):\n",
    "        self.etas = []\n",
    "        self.coherences_e = []\n",
    "\n",
    "        for i in tqdm(range(0, 11), desc='eta'):\n",
    "            eta = i * 0.01\n",
    "            model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, \\\n",
    "                iterations=self.iterations, passes=self.passes, \\\n",
    "                alpha=alpha, eta=eta, eval_every=self.eval_every, random_state=self.random_state)\n",
    "        \n",
    "            cm = CoherenceModel(model=model, corpus=corpus, coherence='c_v')\n",
    "            coherence = cm.get_coherence()\n",
    "\n",
    "            self.coherences_e.append(coherence)\n",
    "            self.etas.append(eta)\n",
    "    \n",
    "    def plot_coherence(self, title='Coherence per Topic Num', root='./'):\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(self.num_topics_c, self.coherences)\n",
    "\n",
    "        plt.xlabel('Topic Number')\n",
    "        plt.ylabel('Coherence')\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.savefig(root+title+'.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_perplexity(self, title='Perplexity per Topic Num', root='./'):\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(self.num_topics_p, self.perplexities)\n",
    "\n",
    "        plt.xlabel('Topic Number')\n",
    "        plt.ylabel('Perplexity')\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.savefig(root+title+'.png')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_alpha(self, title='Coherence per alpha', root='./'):\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(self.alphas, self.coherences_a)\n",
    "\n",
    "        plt.xlabel('alpha')\n",
    "        plt.ylabel('coherence')\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.savefig(root+title+'.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_eta(self, title='Coherence per eta', root='./'):\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(self.etas, self.coherences_e)\n",
    "\n",
    "        plt.xlabel('eta')\n",
    "        plt.ylabel('coherence')\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.savefig(root+title+'.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_param = BestLDAPram(passes=PASSES, iterations=ITERATIONS, eval_every=EVAL_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corp, Dict = buildDTM(wo_stopword_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coherence와 Perplexity로 최적의 topic number 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_param.calc_coherence(corpus=Corp, dictionary=Dict)\n",
    "lda_param.calc_perplexity(corpus=Corp, dictionary=Dict)\n",
    "\n",
    "lda_param.plot_coherence('[Period 1] Coherence per Topic Number', root=RESULT_1)\n",
    "lda_param.plot_perplexity('[Period 1] Perplexity per Topic Number', root=RESULT_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 최적의 Topic number를 이용하여 최적의 alpha와 eta값 추정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 9\n",
    "\n",
    "lda_param.calc_alpha(corpus=Corp, dictionary=Dict, num_topics=NUM_TOPICS)\n",
    "lda_param.calc_eta(corpus=Corp, dictionary=Dict, num_topics=NUM_TOPICS)\n",
    "\n",
    "lda_param.plot_alpha(title='[Period 1] Coherence per alpha', root=RESULT_1)\n",
    "lda_param.plot_eta(title='[Period 1] Coherence per eta', root=RESULT_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 9\n",
    "ALPHA = 0.002\n",
    "ETA = 0.07\n",
    "\n",
    "Corp, Dict = buildDTM(wo_stopword_1)\n",
    "model = models.ldamodel.LdaModel(Corp, id2word=Dict, num_topics=NUM_TOPICS, alpha=ALPHA, eta=ETA, random_state=random_state)\n",
    "topicdf = topicWords(model, NUM_TOPIC_WORDS)\n",
    "topicdf.to_csv(RESULT_1+'[Period 1] topic words.csv', index=False)\n",
    "data = visualizeLDA(model, Corp, Dict)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_param = BestLDAPram(passes=PASSES, iterations=ITERATIONS, eval_every=EVAL_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corp, Dict = buildDTM(wo_stopword_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coherence와 Perplexity로 최적의 topic number 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_param.calc_coherence(corpus=Corp, dictionary=Dict)\n",
    "lda_param.calc_perplexity(corpus=Corp, dictionary=Dict)\n",
    "\n",
    "lda_param.plot_coherence('[Period 2] Coherence per Topic Number', root=RESULT_2)\n",
    "lda_param.plot_perplexity('[Period 2] perplexity per Topic Number', root=RESULT_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 8\n",
    "\n",
    "lda_param.calc_alpha(corpus=Corp, dictionary=Dict, num_topics=NUM_TOPICS)\n",
    "lda_param.calc_eta(corpus=Corp, dictionary=Dict, num_topics=NUM_TOPICS)\n",
    "\n",
    "lda_param.plot_alpha('[Period 2] Coherence per alpha', root=RESULT_2)\n",
    "lda_param.plot_eta('[Period 2] Coherence per eta', root=RESULT_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 8\n",
    "ALPHA = 0.002\n",
    "ETA = 0.05\n",
    "\n",
    "Corp, Dict = buildDTM(wo_stopword_2)\n",
    "model = models.ldamodel.LdaModel(Corp, id2word=Dict, num_topics=NUM_TOPICS, alpha=ALPHA, eta=ETA, random_state=random_state)\n",
    "topicdf = topicWords(model, NUM_TOPIC_WORDS)\n",
    "topicdf.to_csv(RESULT_2+'[Period 2] topic words.csv', index=False)\n",
    "data = visualizeLDA(model, Corp, Dict)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_param = BestLDAPram(passes=PASSES, iterations=ITERATIONS, eval_every=EVAL_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corp, Dict = buildDTM(wo_stopword_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_param.calc_coherence(corpus=Corp, dictionary=Dict)\n",
    "lda_param.calc_perplexity(corpus=Corp, dictionary=Dict)\n",
    "\n",
    "lda_param.plot_coherence('[Period 3] Coherence per Topic Number', root=RESULT_3)\n",
    "lda_param.plot_perplexity('[Period 3] perplexity per Topic Number', root=RESULT_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "\n",
    "lda_param.calc_alpha(corpus=Corp, dictionary=Dict, num_topics=NUM_TOPICS)\n",
    "lda_param.calc_eta(corpus=Corp, dictionary=Dict, num_topics=NUM_TOPICS)\n",
    "\n",
    "lda_param.plot_alpha(title='[Period 3] Coherence per alpha', root=RESULT_3)\n",
    "lda_param.plot_eta(title='[Period 3] Coherence per eta', root=RESULT_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "ALPHA = 0.005\n",
    "ETA = 0.04\n",
    "\n",
    "Corp, Dict = buildDTM(wo_stopword_3)\n",
    "model = models.ldamodel.LdaModel(Corp, id2word=Dict, num_topics=NUM_TOPICS, alpha=ALPHA, eta=ETA, random_state=random_state)\n",
    "topicdf = topicWords(model, NUM_TOPIC_WORDS)\n",
    "topicdf.to_csv(RESULT_3+'[Period 3] topic words.csv', index=False)\n",
    "data = visualizeLDA(model, Corp, Dict)\n",
    "\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a3111211be4281f3a8c4a9b25563b8d253df502c7e31f5318895c1792a97cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
